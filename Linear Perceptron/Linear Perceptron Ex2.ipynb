{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwxWcO7fqb-L"
   },
   "source": [
    "# Excercise - Multi-class classification of MNIST using Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcygblmOmQDZ"
   },
   "source": [
    "In binary perceptron, where $\\mathbf{y} \\in \\{-1, +1\\}$, we used to update our weights only for wrongly classified examples.\n",
    "\n",
    "The multi-class perceptron is regarded as a generalization of binary perceptron. Learning through iteration is the same as the perceptron. Weighted inputs are passed through a multiclass signum activation function. If the predicted output label is the same as true label then weights are not updated. However, when predicted output label $\\neq$ true label, then the wrongly classified input example is added to the weights of the correct label and subtracted from the weights of the incorrect label. Effectively, this amounts to ’rewarding’ the correct weight vector, ’punishing’ the misleading, incorrect weight\n",
    "vector, and leaving alone an other weight vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import gif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 223975,
     "status": "ok",
     "timestamp": 1596984132348,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "gNkGLnbjTY-s"
   },
   "outputs": [],
   "source": [
    "# Setting the seed to ensure reproducibility of experiments\n",
    "np.random.seed(11)\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "def one_hot(a): #making the labels in wich the sample doesn't fall as -1\n",
    "  b = -1 * np.ones((a.size, a.max()+1))\n",
    "  b[np.arange(a.size), a] = 1\n",
    "  return b\n",
    "\n",
    "# Loading digits datasets\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "Y = digits.target\n",
    "Y = one_hot(Y)\n",
    "\n",
    "# Adding column of ones to absorb bias b of the hyperplane into X\n",
    "X = digits.data\n",
    "bias_ones = np.ones((len(X), 1))\n",
    "X = np.hstack((X, bias_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 223957,
     "status": "ok",
     "timestamp": 1596984132353,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "0BPvc5P8KvrM",
    "outputId": "233f09b1-7641-4c60-c21d-74a2264f8bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (1257, 65) (1257, 10)\n",
      "Validation dataset:  (180, 65) (180, 10)\n",
      "Test dataset:  (360, 65) (360, 10)\n"
     ]
    }
   ],
   "source": [
    "# Train-val-test data\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape,Y_train.shape)\n",
    "print(\"Validation dataset: \", X_val.shape,Y_val.shape)\n",
    "print(\"Test dataset: \", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 223939,
     "status": "ok",
     "timestamp": 1596984132358,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "QPJZdeDtUfoy",
    "outputId": "66a50417-5c21-4158-f029-20ef755e50f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2KVp57S1Zah"
   },
   "source": [
    "#### Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(Y):\n",
    "    idx = np.argmax(Y, axis=1)\n",
    "    a,b = np.shape(Y)\n",
    "    for i in range(a):\n",
    "        Y[i,:]=-1\n",
    "        Y[i,idx[i]]=1\n",
    "        \n",
    "    return(Y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining signum activation function\n",
    "def signum(vec_w_x):\n",
    "  \"\"\" signum activation for perceptron\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "  \"\"\"\n",
    "\n",
    "  vec_w_x[vec_w_x >= 0] = 1\n",
    "  vec_w_x[vec_w_x < 0] = -1\n",
    "  return vec_w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron training algorithm\n",
    "def MultiClass_perceptron_train(X_train, Y_train, weights, learning_rate=1, total_epochs=100):\n",
    "#   \"\"\"Training method for Perceptron.\n",
    "  \n",
    "#   Parameters\n",
    "#   -----------\n",
    "\n",
    "#   X_train: ndarray (num_examples(rows) vs num_features(columns)) (1257,65) : including bias 1 with 64 features\n",
    "#     Input dataset which perceptron will use to learn optimal weights\n",
    "  \n",
    "#   Y_train: ndarray (num_examples(rows) vs class_labels(columns))(1257,10)\n",
    "#     Class labels for input data\n",
    "\n",
    "#   weights: ndarray (num_features vs n_output)(65,10)\n",
    "#     Weights used to train the network and predict on test set\n",
    "\n",
    "#   learning_rate: int\n",
    "#     Learning rate use to learn and update weights\n",
    "  \n",
    "#   total_epochs: int\n",
    "#     Max number of epochs to train the perceptron model\n",
    "#   \"\"\"\n",
    "\n",
    "    n_samples, _ = np.shape(X_train)\n",
    "    history_weights = []\n",
    "    epoch = 1\n",
    "\n",
    "  # Number of missclassified points we would like to see in the train set.\n",
    "  # While training, its value will change every epoch. If m==0, our training \n",
    "  # error will be zero.\n",
    "    m = 1\n",
    "\n",
    "  # If the most recent weights gave 0 misclassifications, break the loop.\n",
    "  # Else continue until total_epochs is completed.\n",
    "    while m != 0 and epoch <= total_epochs:\n",
    "        m = 0\n",
    "\n",
    "        # Compute weighted inputs and predict class labels on training set.\n",
    "        weights_transpose_x = np.dot(X_train, weights) #(1257,65)*(65,10)=(1257,10) including the bias\n",
    "        #weights_transpose_x = signum(weights_transpose_x)  #(1257,10) including the bias\n",
    "        #y_train_out = np.multiply(Y_train, weights_transpose_x)  #(1257,10).*(1257,10) : elementwise multiplication\n",
    "        epoch += 1\n",
    "        \n",
    "        # weight is a Column_Vec of 65x1 and X_train = row vector 1x65\n",
    "        # If the predicted output label is the same as true label then weights are not updated.\n",
    "        # However, when predicted output label  ≠  true label, then the wrongly classified \n",
    "        # input example is added to the weights of the correct label and subtracted from the weights of the incorrect label.\n",
    "        \n",
    "        y_pred=label(weights_transpose_x)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            pred_label = np.ravel(np.argwhere(y_pred[i] == 1))\n",
    "            actual_label = np.ravel(np.argwhere(Y_train[i] == 1))\n",
    "            if pred_label != actual_label:\n",
    "                m += 1\n",
    "                sample = np.reshape(X_train[i,:].T,(-1,1))\n",
    "                #print(pred_label,actual_label,weights[:,pred_label].shape,sample.shape)\n",
    "                weights[:,pred_label] -= (learning_rate) * sample\n",
    "                weights[:,actual_label] += (learning_rate) * sample\n",
    "                \n",
    "        weights = np.round(weights, decimals=4)\n",
    "        print('no. of samples misclassified ',m,'in epoch',epoch-1)\n",
    "\n",
    "        # Append weights to visualize decision boundary later\n",
    "        history_weights.append(weights)\n",
    "\n",
    "    if m == 0 and epoch <= total_epochs:\n",
    "        print(\"Training has stabilized with all points classified in : \", epoch-1,'epochs')\n",
    "    else:\n",
    "        print(f'Training completed at {epoch-1} epochs. {m} misclassified points remain.')\n",
    "\n",
    "    return history_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for train, val, and test set.\n",
    "def get_accuracy(y_predicted, Y_input_set, num_datapoints):\n",
    "  miscls_points = np.argwhere(np.any(y_predicted != Y_input_set, axis=1))\n",
    "  miscls_points = np.unique(miscls_points)\n",
    "  accuracy = (1-len(miscls_points)/num_datapoints)*100\n",
    "  return accuracy\n",
    "\n",
    "def get_prediction(X_input_set, Y_input_set, weights, get_acc=False, model_type='perceptron', predict='no'):\n",
    "\n",
    "  if len(Y_input_set) != 0:\n",
    "    num_datapoints, num_categories = np.shape(Y_input_set)\n",
    "\n",
    "  vec_w_transpose_x = np.dot(X_input_set, weights)\n",
    "\n",
    "  if num_categories > 1: # Multi-class\n",
    "    if model_type == 'perceptron':\n",
    "      y_pred_out = multi_class_signum(vec_w_transpose_x)\n",
    "    elif model_type == 'logreg':\n",
    "      y_pred_out = softmax(X_input_set, vec_w_transpose_x, predict=predict)\n",
    "\n",
    "  else: # Binary class\n",
    "    if model_type == 'perceptron' or model_type == 'LinearDA':\n",
    "      y_pred_out = signum(vec_w_transpose_x)\n",
    "    elif model_type == 'logreg':\n",
    "      y_pred_out = sigmoid(vec_w_transpose_x, predict=predict)\n",
    "\n",
    "  # Both prediction and evaluation\n",
    "  if get_acc:\n",
    "    cls_acc = get_accuracy(y_pred_out, Y_input_set, num_datapoints)\n",
    "    return cls_acc, y_pred_out\n",
    "  \n",
    "  # Only prediction\n",
    "  return y_pred_out\n",
    "# multi-class signum\n",
    "def multi_class_signum(vec_w_x):\n",
    "  \"\"\" Multiclass signum activation.\n",
    "\n",
    "  Parameters\n",
    "  ------------\n",
    "  vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "  \"\"\"\n",
    "\n",
    "  flag = np.all(vec_w_x == 0)\n",
    "\n",
    "  if flag:\n",
    "    return vec_w_x\n",
    "\n",
    "  else:\n",
    "    num_examples, num_outputs = np.shape(vec_w_x)\n",
    "    range_examples = np.array(range(0, num_examples))\n",
    "\n",
    "    zero_idxs = np.argwhere(np.all(vec_w_x == 0, axis=1))\n",
    "    non_zero_examples = np.delete(range_examples, zero_idxs[:, 0])\n",
    "      \n",
    "    signum_vec_w_x = vec_w_x[non_zero_examples]\n",
    "    maxvals = np.amax(signum_vec_w_x, axis=1)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "      idx = np.argwhere(signum_vec_w_x == maxvals[i])[0]\n",
    "      signum_vec_w_x[idx[0], idx[1]] = 1\n",
    "\n",
    "    non_maxvals_idxs = np.argwhere(signum_vec_w_x != 1)\n",
    "    signum_vec_w_x[non_maxvals_idxs[:, 0], non_maxvals_idxs[:, 1]] = -1\n",
    "    vec_w_x[non_zero_examples] = signum_vec_w_x\n",
    "\n",
    "    return vec_w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights with normally distributed numbers between 0 and 1 \n",
    "_, n_features = np.shape(X_train)\n",
    "_, n_outputs = np.shape(Y_train)\n",
    "\n",
    "#weights = np.zeros((n_features, n_outputs))\n",
    "weights = np.random.normal(0, 1, (n_features, n_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of samples misclassified  1140 in epoch 1\n",
      "no. of samples misclassified  540 in epoch 2\n",
      "no. of samples misclassified  1143 in epoch 3\n",
      "no. of samples misclassified  974 in epoch 4\n",
      "no. of samples misclassified  876 in epoch 5\n",
      "no. of samples misclassified  982 in epoch 6\n",
      "no. of samples misclassified  767 in epoch 7\n",
      "no. of samples misclassified  668 in epoch 8\n",
      "no. of samples misclassified  811 in epoch 9\n",
      "no. of samples misclassified  839 in epoch 10\n",
      "no. of samples misclassified  675 in epoch 11\n",
      "no. of samples misclassified  537 in epoch 12\n",
      "no. of samples misclassified  472 in epoch 13\n",
      "no. of samples misclassified  405 in epoch 14\n",
      "no. of samples misclassified  220 in epoch 15\n",
      "no. of samples misclassified  132 in epoch 16\n",
      "no. of samples misclassified  173 in epoch 17\n",
      "no. of samples misclassified  207 in epoch 18\n",
      "no. of samples misclassified  198 in epoch 19\n",
      "no. of samples misclassified  168 in epoch 20\n",
      "no. of samples misclassified  146 in epoch 21\n",
      "no. of samples misclassified  123 in epoch 22\n",
      "no. of samples misclassified  106 in epoch 23\n",
      "no. of samples misclassified  93 in epoch 24\n",
      "no. of samples misclassified  79 in epoch 25\n",
      "no. of samples misclassified  75 in epoch 26\n",
      "no. of samples misclassified  67 in epoch 27\n",
      "no. of samples misclassified  65 in epoch 28\n",
      "no. of samples misclassified  60 in epoch 29\n",
      "no. of samples misclassified  60 in epoch 30\n",
      "no. of samples misclassified  56 in epoch 31\n",
      "no. of samples misclassified  63 in epoch 32\n",
      "no. of samples misclassified  56 in epoch 33\n",
      "no. of samples misclassified  65 in epoch 34\n",
      "no. of samples misclassified  52 in epoch 35\n",
      "no. of samples misclassified  64 in epoch 36\n",
      "no. of samples misclassified  51 in epoch 37\n",
      "no. of samples misclassified  58 in epoch 38\n",
      "no. of samples misclassified  50 in epoch 39\n",
      "no. of samples misclassified  55 in epoch 40\n",
      "no. of samples misclassified  48 in epoch 41\n",
      "no. of samples misclassified  54 in epoch 42\n",
      "no. of samples misclassified  47 in epoch 43\n",
      "no. of samples misclassified  52 in epoch 44\n",
      "no. of samples misclassified  44 in epoch 45\n",
      "no. of samples misclassified  50 in epoch 46\n",
      "no. of samples misclassified  40 in epoch 47\n",
      "no. of samples misclassified  46 in epoch 48\n",
      "no. of samples misclassified  39 in epoch 49\n",
      "no. of samples misclassified  45 in epoch 50\n",
      "no. of samples misclassified  39 in epoch 51\n",
      "no. of samples misclassified  45 in epoch 52\n",
      "no. of samples misclassified  38 in epoch 53\n",
      "no. of samples misclassified  43 in epoch 54\n",
      "no. of samples misclassified  37 in epoch 55\n",
      "no. of samples misclassified  41 in epoch 56\n",
      "no. of samples misclassified  39 in epoch 57\n",
      "no. of samples misclassified  42 in epoch 58\n",
      "no. of samples misclassified  36 in epoch 59\n",
      "no. of samples misclassified  39 in epoch 60\n",
      "no. of samples misclassified  35 in epoch 61\n",
      "no. of samples misclassified  39 in epoch 62\n",
      "no. of samples misclassified  34 in epoch 63\n",
      "no. of samples misclassified  38 in epoch 64\n",
      "no. of samples misclassified  32 in epoch 65\n",
      "no. of samples misclassified  38 in epoch 66\n",
      "no. of samples misclassified  38 in epoch 67\n",
      "no. of samples misclassified  39 in epoch 68\n",
      "no. of samples misclassified  31 in epoch 69\n",
      "no. of samples misclassified  36 in epoch 70\n",
      "no. of samples misclassified  30 in epoch 71\n",
      "no. of samples misclassified  37 in epoch 72\n",
      "no. of samples misclassified  34 in epoch 73\n",
      "no. of samples misclassified  35 in epoch 74\n",
      "no. of samples misclassified  29 in epoch 75\n",
      "no. of samples misclassified  37 in epoch 76\n",
      "no. of samples misclassified  31 in epoch 77\n",
      "no. of samples misclassified  34 in epoch 78\n",
      "no. of samples misclassified  32 in epoch 79\n",
      "no. of samples misclassified  36 in epoch 80\n",
      "no. of samples misclassified  36 in epoch 81\n",
      "no. of samples misclassified  39 in epoch 82\n",
      "no. of samples misclassified  43 in epoch 83\n",
      "no. of samples misclassified  43 in epoch 84\n",
      "no. of samples misclassified  47 in epoch 85\n",
      "no. of samples misclassified  50 in epoch 86\n",
      "no. of samples misclassified  54 in epoch 87\n",
      "no. of samples misclassified  59 in epoch 88\n",
      "no. of samples misclassified  62 in epoch 89\n",
      "no. of samples misclassified  66 in epoch 90\n",
      "no. of samples misclassified  68 in epoch 91\n",
      "no. of samples misclassified  65 in epoch 92\n",
      "no. of samples misclassified  68 in epoch 93\n",
      "no. of samples misclassified  71 in epoch 94\n",
      "no. of samples misclassified  72 in epoch 95\n",
      "no. of samples misclassified  69 in epoch 96\n",
      "no. of samples misclassified  72 in epoch 97\n",
      "no. of samples misclassified  60 in epoch 98\n",
      "no. of samples misclassified  56 in epoch 99\n",
      "no. of samples misclassified  47 in epoch 100\n",
      "no. of samples misclassified  34 in epoch 101\n",
      "no. of samples misclassified  32 in epoch 102\n",
      "no. of samples misclassified  24 in epoch 103\n",
      "no. of samples misclassified  24 in epoch 104\n",
      "no. of samples misclassified  20 in epoch 105\n",
      "no. of samples misclassified  21 in epoch 106\n",
      "no. of samples misclassified  18 in epoch 107\n",
      "no. of samples misclassified  24 in epoch 108\n",
      "no. of samples misclassified  21 in epoch 109\n",
      "no. of samples misclassified  19 in epoch 110\n",
      "no. of samples misclassified  18 in epoch 111\n",
      "no. of samples misclassified  24 in epoch 112\n",
      "no. of samples misclassified  19 in epoch 113\n",
      "no. of samples misclassified  25 in epoch 114\n",
      "no. of samples misclassified  20 in epoch 115\n",
      "no. of samples misclassified  22 in epoch 116\n",
      "no. of samples misclassified  20 in epoch 117\n",
      "no. of samples misclassified  18 in epoch 118\n",
      "no. of samples misclassified  18 in epoch 119\n",
      "no. of samples misclassified  20 in epoch 120\n",
      "no. of samples misclassified  15 in epoch 121\n",
      "no. of samples misclassified  15 in epoch 122\n",
      "no. of samples misclassified  14 in epoch 123\n",
      "no. of samples misclassified  15 in epoch 124\n",
      "no. of samples misclassified  17 in epoch 125\n",
      "no. of samples misclassified  22 in epoch 126\n",
      "no. of samples misclassified  17 in epoch 127\n",
      "no. of samples misclassified  23 in epoch 128\n",
      "no. of samples misclassified  17 in epoch 129\n",
      "no. of samples misclassified  19 in epoch 130\n",
      "no. of samples misclassified  17 in epoch 131\n",
      "no. of samples misclassified  25 in epoch 132\n",
      "no. of samples misclassified  25 in epoch 133\n",
      "no. of samples misclassified  27 in epoch 134\n",
      "no. of samples misclassified  28 in epoch 135\n",
      "no. of samples misclassified  28 in epoch 136\n",
      "no. of samples misclassified  24 in epoch 137\n",
      "no. of samples misclassified  26 in epoch 138\n",
      "no. of samples misclassified  19 in epoch 139\n",
      "no. of samples misclassified  17 in epoch 140\n",
      "no. of samples misclassified  14 in epoch 141\n",
      "no. of samples misclassified  18 in epoch 142\n",
      "no. of samples misclassified  15 in epoch 143\n",
      "no. of samples misclassified  14 in epoch 144\n",
      "no. of samples misclassified  9 in epoch 145\n",
      "no. of samples misclassified  10 in epoch 146\n",
      "no. of samples misclassified  15 in epoch 147\n",
      "no. of samples misclassified  12 in epoch 148\n",
      "no. of samples misclassified  14 in epoch 149\n",
      "no. of samples misclassified  11 in epoch 150\n",
      "no. of samples misclassified  17 in epoch 151\n",
      "no. of samples misclassified  12 in epoch 152\n",
      "no. of samples misclassified  13 in epoch 153\n",
      "no. of samples misclassified  10 in epoch 154\n",
      "no. of samples misclassified  17 in epoch 155\n",
      "no. of samples misclassified  15 in epoch 156\n",
      "no. of samples misclassified  12 in epoch 157\n",
      "no. of samples misclassified  7 in epoch 158\n",
      "no. of samples misclassified  15 in epoch 159\n",
      "no. of samples misclassified  14 in epoch 160\n",
      "no. of samples misclassified  15 in epoch 161\n",
      "no. of samples misclassified  17 in epoch 162\n",
      "no. of samples misclassified  21 in epoch 163\n",
      "no. of samples misclassified  23 in epoch 164\n",
      "no. of samples misclassified  21 in epoch 165\n",
      "no. of samples misclassified  20 in epoch 166\n",
      "no. of samples misclassified  13 in epoch 167\n",
      "no. of samples misclassified  9 in epoch 168\n",
      "no. of samples misclassified  11 in epoch 169\n",
      "no. of samples misclassified  6 in epoch 170\n",
      "no. of samples misclassified  12 in epoch 171\n",
      "no. of samples misclassified  8 in epoch 172\n",
      "no. of samples misclassified  16 in epoch 173\n",
      "no. of samples misclassified  13 in epoch 174\n",
      "no. of samples misclassified  13 in epoch 175\n",
      "no. of samples misclassified  10 in epoch 176\n",
      "no. of samples misclassified  15 in epoch 177\n",
      "no. of samples misclassified  12 in epoch 178\n",
      "no. of samples misclassified  17 in epoch 179\n",
      "no. of samples misclassified  14 in epoch 180\n",
      "no. of samples misclassified  12 in epoch 181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of samples misclassified  5 in epoch 182\n",
      "no. of samples misclassified  6 in epoch 183\n",
      "no. of samples misclassified  9 in epoch 184\n",
      "no. of samples misclassified  6 in epoch 185\n",
      "no. of samples misclassified  7 in epoch 186\n",
      "no. of samples misclassified  6 in epoch 187\n",
      "no. of samples misclassified  7 in epoch 188\n",
      "no. of samples misclassified  16 in epoch 189\n",
      "no. of samples misclassified  18 in epoch 190\n",
      "no. of samples misclassified  14 in epoch 191\n",
      "no. of samples misclassified  6 in epoch 192\n",
      "no. of samples misclassified  5 in epoch 193\n",
      "no. of samples misclassified  4 in epoch 194\n",
      "no. of samples misclassified  8 in epoch 195\n",
      "no. of samples misclassified  11 in epoch 196\n",
      "no. of samples misclassified  15 in epoch 197\n",
      "no. of samples misclassified  17 in epoch 198\n",
      "no. of samples misclassified  17 in epoch 199\n",
      "no. of samples misclassified  13 in epoch 200\n",
      "no. of samples misclassified  13 in epoch 201\n",
      "no. of samples misclassified  11 in epoch 202\n",
      "no. of samples misclassified  11 in epoch 203\n",
      "no. of samples misclassified  6 in epoch 204\n",
      "no. of samples misclassified  5 in epoch 205\n",
      "no. of samples misclassified  3 in epoch 206\n",
      "no. of samples misclassified  7 in epoch 207\n",
      "no. of samples misclassified  4 in epoch 208\n",
      "no. of samples misclassified  4 in epoch 209\n",
      "no. of samples misclassified  3 in epoch 210\n",
      "no. of samples misclassified  5 in epoch 211\n",
      "no. of samples misclassified  8 in epoch 212\n",
      "no. of samples misclassified  8 in epoch 213\n",
      "no. of samples misclassified  12 in epoch 214\n",
      "no. of samples misclassified  15 in epoch 215\n",
      "no. of samples misclassified  14 in epoch 216\n",
      "no. of samples misclassified  14 in epoch 217\n",
      "no. of samples misclassified  12 in epoch 218\n",
      "no. of samples misclassified  11 in epoch 219\n",
      "no. of samples misclassified  8 in epoch 220\n",
      "no. of samples misclassified  8 in epoch 221\n",
      "no. of samples misclassified  5 in epoch 222\n",
      "no. of samples misclassified  2 in epoch 223\n",
      "no. of samples misclassified  3 in epoch 224\n",
      "no. of samples misclassified  4 in epoch 225\n",
      "no. of samples misclassified  3 in epoch 226\n",
      "no. of samples misclassified  5 in epoch 227\n",
      "no. of samples misclassified  2 in epoch 228\n",
      "no. of samples misclassified  8 in epoch 229\n",
      "no. of samples misclassified  6 in epoch 230\n",
      "no. of samples misclassified  11 in epoch 231\n",
      "no. of samples misclassified  14 in epoch 232\n",
      "no. of samples misclassified  16 in epoch 233\n",
      "no. of samples misclassified  18 in epoch 234\n",
      "no. of samples misclassified  20 in epoch 235\n",
      "no. of samples misclassified  22 in epoch 236\n",
      "no. of samples misclassified  19 in epoch 237\n",
      "no. of samples misclassified  13 in epoch 238\n",
      "no. of samples misclassified  6 in epoch 239\n",
      "no. of samples misclassified  3 in epoch 240\n",
      "no. of samples misclassified  4 in epoch 241\n",
      "no. of samples misclassified  2 in epoch 242\n",
      "no. of samples misclassified  3 in epoch 243\n",
      "no. of samples misclassified  3 in epoch 244\n",
      "no. of samples misclassified  4 in epoch 245\n",
      "no. of samples misclassified  4 in epoch 246\n",
      "no. of samples misclassified  10 in epoch 247\n",
      "no. of samples misclassified  9 in epoch 248\n",
      "no. of samples misclassified  6 in epoch 249\n",
      "no. of samples misclassified  3 in epoch 250\n",
      "no. of samples misclassified  4 in epoch 251\n",
      "no. of samples misclassified  2 in epoch 252\n",
      "no. of samples misclassified  6 in epoch 253\n",
      "no. of samples misclassified  8 in epoch 254\n",
      "no. of samples misclassified  8 in epoch 255\n",
      "no. of samples misclassified  9 in epoch 256\n",
      "no. of samples misclassified  12 in epoch 257\n",
      "no. of samples misclassified  15 in epoch 258\n",
      "no. of samples misclassified  21 in epoch 259\n",
      "no. of samples misclassified  22 in epoch 260\n",
      "no. of samples misclassified  25 in epoch 261\n",
      "no. of samples misclassified  19 in epoch 262\n",
      "no. of samples misclassified  22 in epoch 263\n",
      "no. of samples misclassified  18 in epoch 264\n",
      "no. of samples misclassified  13 in epoch 265\n",
      "no. of samples misclassified  7 in epoch 266\n",
      "no. of samples misclassified  3 in epoch 267\n",
      "no. of samples misclassified  4 in epoch 268\n",
      "no. of samples misclassified  3 in epoch 269\n",
      "no. of samples misclassified  6 in epoch 270\n",
      "no. of samples misclassified  4 in epoch 271\n",
      "no. of samples misclassified  6 in epoch 272\n",
      "no. of samples misclassified  7 in epoch 273\n",
      "no. of samples misclassified  6 in epoch 274\n",
      "no. of samples misclassified  2 in epoch 275\n",
      "no. of samples misclassified  4 in epoch 276\n",
      "no. of samples misclassified  3 in epoch 277\n",
      "no. of samples misclassified  4 in epoch 278\n",
      "no. of samples misclassified  7 in epoch 279\n",
      "no. of samples misclassified  6 in epoch 280\n",
      "no. of samples misclassified  11 in epoch 281\n",
      "no. of samples misclassified  8 in epoch 282\n",
      "no. of samples misclassified  6 in epoch 283\n",
      "no. of samples misclassified  3 in epoch 284\n",
      "no. of samples misclassified  3 in epoch 285\n",
      "no. of samples misclassified  2 in epoch 286\n",
      "no. of samples misclassified  4 in epoch 287\n",
      "no. of samples misclassified  5 in epoch 288\n",
      "no. of samples misclassified  4 in epoch 289\n",
      "no. of samples misclassified  3 in epoch 290\n",
      "no. of samples misclassified  1 in epoch 291\n",
      "no. of samples misclassified  2 in epoch 292\n",
      "no. of samples misclassified  0 in epoch 293\n",
      "Training has stabilized with all points classified in :  293 epochs\n"
     ]
    }
   ],
   "source": [
    "trained_weights = MultiClass_perceptron_train(X_train, Y_train, weights, learning_rate=0.1, total_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results\n",
      "Training accuracy: 100.000\n",
      "Validation accuracy: 94.444\n",
      "Test accuracy: 95.556\n"
     ]
    }
   ],
   "source": [
    "best_weights = trained_weights[-1]\n",
    "train_acc, _ = get_prediction(X_train, Y_train, best_weights, get_acc=True)\n",
    "val_acc, _ = get_prediction(X_val, Y_val, best_weights, get_acc=True)\n",
    "test_acc, _ = get_prediction(X_test, Y_test, best_weights, get_acc=True)\n",
    "\n",
    "print(\"Evaluation results\")\n",
    "print(\"Training accuracy: {:.3f}\" .format(train_acc))\n",
    "print(\"Validation accuracy: {:.3f}\" .format(val_acc))\n",
    "print(\"Test accuracy: {:.3f}\" .format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZQQfFFOrqST3"
   ],
   "name": "LinearPerceptron_draft4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
