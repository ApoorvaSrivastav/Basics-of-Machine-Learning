{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization using Gradient Descent\n",
    "\n",
    "In this excercise, you are required to implement matrix factorization method, specifically [Non-Negative Matrix Factorization (NMF)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization), using gradient descent. You have to apply the matrix factorization to solve topic modeling. \n",
    "\n",
    "(Please refer to the tutorial on basics of topic modeling, LSI with SVD (Tutorial Set 4), for details on LSI)\n",
    "\n",
    "## Applying NMF to solve Topic Modeling\n",
    "Given a term document matrix $V$, NMF factorizes it into two matrix $W$ and $H$ with the property that all three documents have no negative elements.\n",
    "<img src=\"content/NMF.png\" alt=\"Non-negative matrix factorization\" style=\"width: 80%\">\n",
    "\n",
    "In Non-negative Matrix Factorization, a document-term matrix is approximately factorized into term-feature and feature-document matrices.\n",
    "\n",
    "$V = WH$ Matrix multiplication can be implemented as computing the column vectors of $V$ as linear combinations of the basis vectors (column vectors) in $W$ (or the topics discovered from the documents) using coefficients supplied by columns of $H$ (or the membership weights for the topics in each document). That is, each column of V can be computed as follows:\n",
    "$$ v_i = W h_i$$\n",
    "\n",
    "In what follows, we will first see an example of applying NMF by using [SKLearn NMF API](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) for the task of topic modeling. Later you will be required to implement NMF using gradient descent.\n",
    "\n",
    "### Scikit-Learn implementation of NMF for topic modeling\n",
    "Given a set of multivariate  $n$-dimensional data vectors, they are put into an  $n\\times m$  matrix  $V$  as its columns, where  $m$  is the number of examples in the data set. This matrix  $V$  is approximately factorized into an  $n \\times t$  matrix  $W$  and an  $t \\times m$  matrix  $H$ , where  $t$  is generally less than  $n$  or  $m$ . Hence, this results in a compression of the original data matrix.\n",
    "\n",
    "In terms of topic modeling, the input document-term matrix  $V$  is factorized into a  $n \\times t$  document-topic matrix and a  $t \\times m$  topic-term matrix, where  $t$  is the number of topics produced. Similar to tutorial 4, we will be using 20 NewsFetch dataset for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute document features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute NMF using Scikit Learn library\n",
    "\n",
    "We will also write a function to display top 8 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "vocab = np.array(vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "d = 5 # num topics\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people don think just like objective say morality',\n",
       " 'graphics thanks files image file program windows know',\n",
       " 'space nasa launch shuttle orbit moon lunar earth',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens',\n",
       " 'god jesus bible believe christian atheism does belief']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF using SGD\n",
    "\n",
    "In stochastic gradient descent (SGD), we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it's much more efficient!\n",
    "\n",
    "### Applying SGD to NMF\n",
    "\n",
    "Goal: Decompose $V\\;(m \\times n)$ into\n",
    "$$ V \\approx HW$$\n",
    "where $W\\;(m \\times d)$ and $H\\;(d \\times n)$, $W,\\;H\\; \\geq \\;0$, and we've minimized the Frobenius norm of $V-WH$. The objective function can therefore be written as the following:\n",
    "$$\n",
    "\\min_{H \\geq 0, W \\geq 0} F(H,W) = \\frac{1}{2} ||V - HW||^{2} + \\frac{\\lambda}{2} \\left( ||H||^2 + ||W||^2 \\right)\n",
    "$$\n",
    "\n",
    "### Implementation of NMF using SGD (Excercise)\n",
    "__Approach:__ Given the objective function above, pick random positive $W$ & $H$, and then use SGD to optimize. \n",
    "\n",
    "(Note that the objective function is non-convex in nature, and is convex only if we consider $H$ and $W$ separately. You can directly write the gradient descent rule for the objective function presented above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating the value of forbenius norm objective function \n",
    "def lossfunc(Mat,W,H,lamda):\n",
    "    Mat_pred=np.matmul(W,H)\n",
    "    loss = 0.5 * np.square(np.linalg.norm(Mat-Mat_pred)) + 0.5 * lamda * (np.square(np.linalg.norm(W)) + np.square(np.linalg.norm(Mat-Mat_pred)))\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads(Mat,W,H,lamda):\n",
    "    grad_w= np.matmul((np.matmul(W,H) - Mat), H.T) + lamda*W\n",
    "    grad_h= np.matmul(W.T, (np.matmul(W,H) - Mat)) + lamda*H\n",
    "    return(grad_w,grad_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' WRITE YOUR CODE BELOW '''\n",
    "def sgd(Mat,num_topics,eta=0.1,lamda=0.5,epsilon=1e-05):\n",
    "    #The input document-term matrix Mat mxn is factorized into a  m√óùë°  document-topic matrix(W)\n",
    "    #and a  ùë°√ón  topic-term matrix(H), where t is the number of topics produced using SGD\n",
    "    i=0\n",
    "    Error2=0\n",
    "    w_history = []\n",
    "    h_history = []\n",
    "    loss_history = []\n",
    "    m,n =Mat.shape\n",
    "    # Initializing weights\n",
    "    W = np.abs(np.random.normal(scale=0.01, size=(m,num_topics)))\n",
    "    H = np.abs(np.random.normal(scale=0.01, size=(num_topics,n)))\n",
    "    # Applying SGD\n",
    "    while(1):\n",
    "        w_history.append(W)\n",
    "        h_history.append(H)\n",
    "        w_k=W\n",
    "        h_k=H\n",
    "        # Loss and Gradient calculaation\n",
    "        Loss= lossfunc(Mat,W,H,lamda)\n",
    "        loss_history.append(Loss)\n",
    "        grad_w,grad_h= grads(Mat,W,H,lamda) \n",
    "        #update\n",
    "        W = W - eta*grad_w\n",
    "        H = H - eta*grad_h\n",
    "        i+=1\n",
    "        Error1=Error2\n",
    "        Error2 = np.abs(np.mean(Mat -W@H))\n",
    "        print('Error in iteration',i,'is',Error2)\n",
    "        if  (np.abs(Error1-Error2)) < epsilon: #If error is not decreasing more than epsilon in 2 consecutive iterations\n",
    "            break\n",
    "    return(W,H,loss_history,h_history,w_history,i,Error2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in iteration 1 is 8.562116888746226e-05\n",
      "Error in iteration 2 is 7.26244502316781e-05\n",
      "Error in iteration 3 is 6.178388829927012e-05\n",
      "Error in iteration 4 is 5.277025363245141e-05\n",
      "Error in iteration 5 is 4.5315801024460875e-05\n",
      "Error in iteration 6 is 3.92002458590754e-05\n",
      "Error in iteration 7 is 3.424053011063114e-05\n",
      "Final Error 3.424053011063114e-05 \n",
      " Loss: [1480.4515720763334, 1479.882280976941, 1479.4174072549167, 1479.0301763231987, 1478.7010158296341, 1478.4154197870255, 1478.1625082362243] \n",
      " Total number of iterations before convergence: 7\n"
     ]
    }
   ],
   "source": [
    "Mat=vectors_tfidf\n",
    "num_topics = 5\n",
    "W_mat,H_mat,loss_history,h_history,w_history,num_iterations,Error = sgd(Mat,num_topics,eta=0.01,lamda=0.5,epsilon=5e-06)\n",
    "\n",
    "print ('Final Error',Error,'\\n','Loss:',loss_history,'\\n','Total number of iterations before convergence:',num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26576) (2034, 5) (5, 26576) Mat-H_mat*W_mat= 3.424053011063114e-05\n"
     ]
    }
   ],
   "source": [
    "print (Mat.shape,W_mat.shape,H_mat.shape,'Mat-H_mat*W_mat=',Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
