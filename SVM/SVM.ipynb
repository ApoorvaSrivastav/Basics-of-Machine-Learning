{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem  (Assignment 5, TAO, Spring 2019)\n",
    "### Instructor: Dr. Pawan Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Problem\n",
    "### Given a set of input vectors corresponding to objects (or featues) decide which of the N classes the object belogs to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference (some figures for illustration below are taken from this): \n",
    "1. SVM without Tears, https://med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Methods for Classification Problem\n",
    "1. Perceptron\n",
    "2. SVM\n",
    "3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will briefly describe the idea behind support vector machines for classification problems. We first describe linear SVM used to classify linearly separable data, and then we describe how we can use these algorithm for non-linearly separable data by so called kernels. The kernels are functions that map non-linearly separable data to a space usually higher dimensional space where the data becomes linearly separable. Let us quickly start with linear SVM.\n",
    "\n",
    "### Linear SVM for two class classification\n",
    "We recall the separating hyperpplane theorem: If there are two non-intersecting convex set, then there exists a hyperplane that separates the two convex sets. This is the assumption we will make: we assume that the convex hull of the given data leads to two convex sets for two classes, such that a hyperplane exists that separates the convex hulls. \n",
    "\n",
    "### Main idea of SVM: \n",
    "Not just find a hyperplane (as in perceptrons), but find one that keeps a good (largest possible) gap from the the data samples of each class. This gap is popularly called margins.\n",
    "\n",
    "### Illustration of problem, and kewords\n",
    "Consider the dataset of cancer and normal patients, hence it is a two class problem. Let us visualize the data:\n",
    "<img src=\"svmt1.png\" width=\"550\">\n",
    "Let us notice the following about the given data:\n",
    "0. There are two classes: blue shaded stars and red shaded circles.\n",
    "2. The input vector is two dimensional, hence it is of the form $(x_1^1, x_2^1).$\n",
    "2. Here $x_1^1, x_2^2$ are values of the features corresponding to two gene features: Gene X, Gene Y.\n",
    "3. Here red line is the linear classifier or hyperplane that separates the given input data.\n",
    "4. There are two dotted lines: one passes through a blue star point, and another dotted line passes through two red shaded circle points.\n",
    "5. The distance between the two dotted lines is called gap or margin that we mentioned before.\n",
    "6. Goal of SVM compared to perceptrons is to maximize this margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of Optimization Model for Linear SVM\n",
    "We now assume that the red hyperplane above with maximum margin is given by $$w \\cdot x + b,$$\n",
    "We further assume that the dotted lines above are given by $$w \\cdot x + b = -1, \\quad w \\cdot x + b = +1.$$\n",
    "<img src=\"svmt2.png\" width=\"400\">\n",
    "For reasons, on why we can choose such hyperplane is shown in slides Lecture 16 of TAO. Since we want to maximize the margin the distance between the dotted lines, we recall the formula for diatance between planes. Let $D$ denote the distance, then \n",
    "$$D = 2/ \\| w \\|.$$\n",
    "So, to maximize the margin $D,$ we need to minimize $\\| w \\|.$ For convenience of writing algorithm (for differentiable function), we can say that minimizing $\\| w \\|$ is equivalent to minimizing $1/2 \\| w \\|^2.$ Hence \n",
    "### Objective function: $\\dfrac{1}{2} \\| w \\|^2$\n",
    "For our hyperplane to classify correctly, we need points of one class on one side of dotted line, more concretely\n",
    "$$w \\cdot x + b \\leq -1,$$\n",
    "and the we want the samples of another class (red ones) be on the other side of other dotted lines, i.e., \n",
    "$$ w \\cdot x + b \\geq +1.$$\n",
    "Let us now look what constraints mean in figure:\n",
    "<img src=\"svmt3.png\" width=\"400\">\n",
    "With this we are all set to write the constraints for our optimization model for SVM. \n",
    "### Constraints: \n",
    "$$\n",
    "\\begin{align}\n",
    "&w \\cdot x_i + b \\leq -1, \\quad \\text{if}~y_i = -1\\\\\n",
    "&w \\cdot x_i + b \\geq +1, \\quad \\text{if}~y_i = +1\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, objective function with constraints, gives us the full model. The data for which the label $y_i$ is $-1$ satisfies $w \\cdot x + b \\leq -1,$ and the data for which the lable $y_i$ is $+1$ satisfies $w \\cdot x + b \\geq +1.$ Hence both these conditions can be combined to get\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i (w \\cdot x_i + b) \\geq 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Optimization Model (Primal Form):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} \\quad & \\dfrac{1}{2} \\| w \\|^2 \\\\\n",
    "\\text{subject to} \\quad &w \\cdot x_i + b \\geq 1, \\quad i=1,\\dots,m,\n",
    "\\end{align}\n",
    "$$\n",
    "where $m$ is the number of samples $x_i,$ and $w \\in \\mathbb{R}^n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Prove that the primal objective is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Primal_Objective_Convex.jpeg)\n",
    "![](Primal2.jpeg)\n",
    "![](Primal3.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Write the primal problem in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Primal_Standard.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model (Dual Form)\n",
    "The dual form was derived in lecture 16:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m{\\lambda_i} - \\dfrac{1}{2} \\sum_{i=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m{\\lambda_i y_i} = 0, \\quad i = 1, \\dots, m\n",
    "\\end{align*}, \n",
    "$$\n",
    "where $\\lambda_i$ is the Lagrange multiplier. We claim that strong duality holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Show the derivation of dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your answer here (use latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$minimize \\: 1/2\\left \\| w \\right \\|^{2}\\\\\n",
    "such that  \\: \\: \\:  -y{_{i}}(w_{i}x_{i}+b-1)\\leq 0 \\: \\: \\: \\: \\:   i:1\\rightarrow m\\\\\n",
    "Defining Lagrangian:\\\\\n",
    "L(w,b,\\Lambda )=1/2\\left \\| w \\right \\|^{2} + \\sum_{i=1}^{m}\\Lambda _{i}(-y^{_{i}}(w_{i}x_{i}+b-1))\\\\The Dual Function,is\\\\\n",
    "g(\\Lambda )=inf_{w,b}\\, L(w,b,\\Lambda )\\\\\n",
    "as\\,  L(w,b,\\Lambda ) is\\,  convex\\, in\\, w,b\\, minima\\, is\\, given\\, by\\\\\n",
    "\\nabla{_{w,b}}L(w,b,\\Lambda )=0\\\\\n",
    "\\frac{\\partial L(w,b,\\Lambda )}{\\partial b}=0\\\\\n",
    "\\frac{\\partial 1/2\\left \\| w \\right \\|^{2} + \\sum_{i=1}^{m}\\Lambda _{i}(-y^{_{i}}(w_{i}x_{i}+b-1))}\n",
    "{\\partial b}=0\\\\\n",
    "\\sum_{i=1}^{m}\\Lambda _{i}y_{i} =0\\\\\n",
    "\\frac{\\partial L(w,b,\\Lambda )}{\\partial w}=0\\\\\n",
    "\\frac{\\partial 1/2\\left \\| w \\right \\|^{2}  + \\sum_{i=1}^{m}\\Lambda _{i}(-y^{_{i}}(w_{i}x_{i}+b-1))}\n",
    "{\\partial w}=0\\\\\n",
    "w= \\sum_{i=1}^{m}\\Lambda _{i}y_{i}x_{i}\\\\\n",
    "Substituting \\: the \\: above\\: two \\:conditions\\: in\\:Lagrangian \\:we\\: obtain\\:Dual problem\\\\\n",
    "maximize:\\\\ \n",
    "g(\\Lambda )=\\sum_{i=1}^{m}\\Lambda _{i}\\: -1/2\\sum_{i=1}^{m}\\Lambda _{i}\\Lambda_{j}y_{i}y_{j}(x_{i}.x{j})\\\\\n",
    "subject\\: to\\\\\n",
    "\\lambda_{i}\\geq 0;\\\\\n",
    "\\sum_{i=1}^{m}\\Lambda _{i}y{i}=0;\\: \\: i:1\\rightarrow m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Prove that strong duality holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your answer here (use latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Strong Duality holds if\\\\\n",
    "1) \\:Primal\\: Problem\\: is \\:convex\\\\\n",
    "2)\\:Slater's \\:Condition \\:holds \\: when\\:  some\\:  f_{i}\\:  are \\: affine\\:  i.e.\\\\\n",
    "there \\: exists \\: x\\in int D \\: with\\\\\n",
    "f_{x}\\leq 0,\\:\\:i=k+1,....,m,\\:\\: Ax=b\\\\\n",
    "For\\:SVM \\:primal \\:problem\\\\\n",
    " objective\\:\\: is\\: quadratic\\: and\\: convex,\\: and \\:all\\: inequality\\: constraints\\: are\\: affine\\\\\n",
    " Slater's \\:condition\\: holds \\:trivially, \\:hence, \\:strong\\: duality \\:holds$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Prove that the dual objective is concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$The \\:Lagrange\\: dual \\:function:\\: minimize \\:Lagrangian\\: (lower bound)\\\\\n",
    "g(λ, ν) = inf_{x∈D}L(x, λ, ν)\\\\\n",
    "Dual \\:function\\: is \\:a \\:pointwise\\: infimum\\: of \\:affine\\: functions \\:of \\:(λ, ν),\\:\n",
    "hence \\:concave \\:in\\: (λ, ν) \\:with \\:convex\\: constraint \\:set λ \\geq 0.\n",
    "$\n",
    "![](ConcaveProof.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Write the dual problem in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Dual_Standard.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin SVM\n",
    "In a variant of soft margin SVM, we assume that some data samples may be outliers or noise, and this prevents the data from being linearly separable. For example, see the figure below\n",
    "<img src=\"svmt4.png\" width=\"400\">\n",
    "In the figure, we see that \n",
    "\n",
    "- We believe that two red and one blue sample is noisy or outliers.\n",
    "- We now want to take into account that real life data is noisy, we decide to allow for some of the noisy data in the margin.\n",
    "- Let $\\xi_i$ denotes how far a data sample is from the middle plane (margin is the area between dotted line).\n",
    "- For example, one of the noisy red data point in 0.6 away from middle red plane. \n",
    "- We introduce this slack variable $\\xi_i \\geq 0$ for each data sample $x_i.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Primal Soft-Margin\n",
    "We can then write the primal soft-margin optimization model as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\dfrac{1}{2} \\| w \\|^2 + C \\sum_{i=1}^m \\xi_i \\\\\n",
    "&\\text{subject to} \\quad y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\dots, m.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Dual Soft-Margin\n",
    "We can also write the dual form of soft-margin SVM as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\dfrac{1}{2} \\sum_{i,j=1}^m \\lambda_i \\lambda_j \\: y_i y_j \\: x_i \\cdot x_j \\\\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad  i=1, \\dots, m, \\\\ \n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\t \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Show the derivation of dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your anser here (use latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SVM Soft Margin \\\\\n",
    "1/2\\left \\| w \\right \\|^{2}\\: + \\: C/2*(\\zeta_{i}) ^{2} \\\\\n",
    "such\\: that \\\\\n",
    "y_{i}(wx_{i}+b)-1 + \\zeta_{i}\\geq 0\\\\\n",
    "Applying Lagrange's Rule,\\\\\n",
    "L= 1/2 \\left \\| w \\right \\|^{2} +C/2\\sum_{i=1}^{m}\\zeta_{i}) ^{2}  -\\sum_{i=1}^{m}\\alpha _{i}[y_{i}(wx_{i}+b)-1 + \\zeta_{i}]-\\sum_{i=1}^{m}\\Lambda _{i}\\zeta _{i}\\\\\n",
    "\\alpha _{i}, \\:and\\: \\Lambda _{i} are\\: Lagrange's Multiplier\\\\\n",
    "\\frac{\\partial L}{\\partial w}=w-\\sum_{i=1}^{m}\\alpha _{i}y_{i}x_{i}=0\\\\\n",
    "\\frac{\\partial L}{\\partial b}=\\sum_{i=1}^{m}\\alpha _{i}y_{i}=0\\\\\n",
    "\\frac{\\partial L}{\\partial \\zeta_{i}}=C\\zeta_{i}-\\alpha{i}-\\Lambda {i}=0\\\\\n",
    "Putting \\:above\\: 3\\: equations \\:in\\: L\\\\\n",
    "L=-1/2\\sum_{i,j=1}^{m}\\alpha _{i}\\alpha_{j}y_{i}y_{j}x_{i}x_{j} \\:+\\:1/2(-\\sum_{i=1}^{m}\\alpha_{i}\\:-\\sum_{i=1}^{m}\\Lambda _{i}\\:+C\\sum_{i}^{m}\\zeta _{i})\\zeta _{i}\\:-1/2\\sum_{i=1}^{m}\\alpha_{i}\\zeta_{i}\\:-1/2\\sum_{i=1}^{m}\\lambda_{i}\\zeta_{i}\\:\\sum_{i=1}^{m}\\alpha_{i}\\\\\n",
    "\\:\\:L=\\sum \\alpha_{i}\\:-1/2\\sum_{i,j=1}^{m}\\alpha _{i}\\alpha_{j}y_{i}y_{j}x_{i}x_{j} \\:-1/2\\sum_{i=1}^{m}(\\alpha_{i}\\:+\\Lambda _{i})\\zeta_{i}\\\\\n",
    "\\zeta_{i}=\\:(\\alpha_{i}\\:+\\Lambda _{i})/C\\\\L= \\sum \\alpha _{i}\\: -1/2\\sum_{i=1}^{m}\\alpha _{i}\\alpha_{j}y_{i}y_{j}x_{i}x_{j} - 1/2\\sum_{i=1}^{m}(\\alpha_{i}\\:+\\Lambda _{i})^{2}/C\\\\\n",
    "s.t.\\: \\: \\sum\\alpha_{i}y{i}=0\\\\\n",
    "C\\zeta_{i}-\\alpha_{i}-\\lambda_{i}=0 \\: is \\:the\\: Dual\\: Form\\: of\\: the\\: Soft\\: Margin\\: SVM\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ List advantages of dual over primal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Put your answer here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Dual Problem over Primal in SVM:\n",
    "1)It allow us to apply kernels.Kernel search an optimal separating hyperplane in a higher dimensional space without increasing the computational complexity much.\n",
    "2)There are some algorithms like SMO(Sequential Minimal Optimization) solves the dual problem efficiently.\n",
    "3)No need to access original data, need to access only dot products.\n",
    "4)Number of free parameters is bounded by the number of support vectors and not by the number of variables (beneficial for high-dimensional problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels in SVM\n",
    "## Non-Linear Classifiers\n",
    "- For nonlinear data, we may map the data to a higher dimensional feature space where it is separable. See the figure below:\n",
    "<img src=\"svmt5.png\" width=\"700\">\n",
    "Such non-linear transformation can be implemented more effectively using the dual formulation. \n",
    "- If we solve the dual form of linear SVM, then the predictions is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\text{sign}(w \\cdot x + b) \\\\\n",
    "w &= \\sum_{i=1}^m \\alpha_i y_i x_i \n",
    "\\end{align*}\n",
    "$$\n",
    "If we assume that we did some transform $\\Phi,$ then the classifier is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\text{sign} (w \\cdot \\Phi(x) + b) \\\\\n",
    "w &= \\sum_{i=1}^m \\alpha_i y_i \\Phi(x_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "If we substitute $w$ in $f(x),$ we observe that\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) = \\text{sign} \\left ( \\sum_{i=1}^m \\alpha_i y_i \\, \\Phi(x_i) \\cdot \\Phi(x) + b \\right) = \\text{sign} \\left( \\sum_{i=1}^m \\alpha_i y_i \\, K(x_i, x) + b \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "Note that doing dot products such as $\\Phi(x_i) \\cdot \\Phi(x),$ if $\\Phi(x)$ is a long vector! An important observation is to define this dot product or $K(x,z)$ such that dot products happen in input space rather than the feature space. We can see this with following example:\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(x \\cdot z) &= (x \\cdot z)^2 = \\left( \\begin{bmatrix}\n",
    "x_{(1)} \\\\ x_{(2)} \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "z_{(1)} \\\\ z_{(2)}\n",
    "\\end{bmatrix} \\right)^2 = (x_{(1)} z_{(1)} + x_{(2)} z_{(2)})^2 \\\\\n",
    "&= x_{(1)}^2 z_{(1)}^2 + 2x_{(1)} z_{(1)} x_{(2)} z_{(2)} + x_{(2)}^2 z_{(2)}^2 = \\begin{bmatrix}\n",
    "x_{(1)}^2 \\\\ \\sqrt{2} x_{(1)} x_{(2)} \\\\ x_{(2)}^2 \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "z_{(1)}^2 \\\\ \\sqrt{2} z_{(1)} z_{(2)} \\\\ z_{(2)}^2 \n",
    "\\end{bmatrix}  \\\\\n",
    "&= \\Phi(x) \\cdot \\Phi(z)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Let the kernel be defined by $K(x,z) = (x \\cdot z)^3.$ Define $\\Phi(x).$ Assuming that one multiplications is 1 FLOP, and one addition is 1 FLOP, then how many flops you need to compute $K(x \\cdot z)$ in input space versus feature space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Write your answer in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](FLOP1.jpeg)\n",
    "![](FLOP2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Dual Soft Margin Kernel SVM\n",
    "We can now write the dual form of soft-margin Kernel SVM as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\dfrac{1}{2} \\sum_{i, \\, j=1}^m \\lambda_i \\lambda_j \\: y_i y_j \\: \\Phi(x_i) \\cdot \\Phi(x_j) \\\\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad  i=1, \\dots, m, \\\\ \n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\t \n",
    "\\end{align*}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver for Optimization Problem: Quadratic Programming\n",
    "We aspire to solve the above optimization problem using existing quadraric programming library. But we have a problem: the standard libraries use the standard form of the quadratic optimization problem that looks like the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad &\\dfrac{1}{2} x^T P x + q^T x, \\\\ \n",
    "\\text{subject to} \\quad &Gx \\leq h, \\\\\n",
    "&Ax = b\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Soft-Margin Kernel SVM in Standard QP: Assemble Matrices Vectors\n",
    "To put the dual Kernel SVM in standard form, we need to set\n",
    "- matrix $P$\n",
    "- vector $x$\n",
    "- vector $q$\n",
    "- vector $h$\n",
    "- vector $b$\n",
    "- matrix $G$\n",
    "- matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $P$\n",
    "Let $$K(x_i, x_j) = \\Phi(x_i) \\cdot \\Phi(x_j),$$ and set $(i,j)$ entry of matrix $P$ as $$P_{ij} = y_iy_j K(x_i,x_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $x$\n",
    "Set $$x = \\begin{bmatrix}\n",
    "\\lambda_1 \\\\\n",
    "\\lambda_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $q$\n",
    "Set $q \\in \\mathbb{R}^m$\n",
    "$$ q = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\ -1 \\\\ \\vdots \\\\ -1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix $A$\n",
    "Set the matrix (in fact vector) $A$ as \n",
    "$$\n",
    "A = [y_1, y_2, \\dots, y_m]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $b$\n",
    "In fact vector $b$ is a scalar here: $$b = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix $G$\n",
    "$$\n",
    "\\begin{align*}\n",
    "G = \\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\dots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 1 \\\\ \\hline\n",
    "-1 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\dots & \\vdots \\\\\n",
    "0 & 0 & \\dots& -1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $h$\n",
    "Set $h$ as \n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "C \\\\\n",
    "C \\\\\n",
    "\\vdots \\\\\n",
    "C \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Kernel SVM\n",
    "We are all set to try out coding the classifier using Kernel SVM. We will first import some libraries. Some of these libraries may not be available in your system. You may install them as follows:\n",
    "- conda install numpy\n",
    "- conda install -c conda-forge cvxopt\n",
    "- sudo apt-get install python-scipy python-matplotlib\n",
    "\n",
    "Try google search, if these does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import cvxopt as cvxopt\n",
    "from cvxopt import solvers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a class: svm \n",
    "This class will have the following functions:\n",
    "- __init__: where we will define initial default parameters\n",
    "- *construct_kernel*: here we will define some kernels such as polynomial and RBF (radial basis or Gaussian kernel)\n",
    "- *train_kernel_svm*: Here we will train, i.e, we will call a quadratic programming solver from cvxopt\n",
    "- *classify*: Here we will test our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ Fill the TODO below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm:\n",
    "\n",
    "    def __init__(self, kernel='linear', C=None, sigma=1., degree=1., threshold=1e-5):\n",
    "        self.kernel = kernel\n",
    "        if self.kernel == 'linear':\n",
    "            self.degree = 1.\n",
    "            self.kernel = 'poly'\n",
    "            \n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.threshold = threshold\n",
    "        self.degree = degree\n",
    "        \n",
    "\n",
    "    def construct_kernel(self, X):\n",
    "        self.K = np.dot(X, X.T)\n",
    "\n",
    "        if self.kernel == 'poly':\n",
    "            self.K = (1. + 1./self.sigma*self.K)**self.degree\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            self.xsquared = (np.diag(self.K)*np.ones((1, self.N))).T\n",
    "            b = np.ones((self.N, 1))\n",
    "            self.K -= 0.5*(np.dot(self.xsquared, b.T) +\n",
    "                           np.dot(b, self.xsquared.T))\n",
    "            self.K = np.exp(self.K/(2.*self.sigma**2))\n",
    "\n",
    "    def train_kernel_svm(self, X, targets):\n",
    "        self.N = np.shape(X)[0]\n",
    "        self.construct_kernel(X)\n",
    "\n",
    "        # Assemble the matrices for the constraints \n",
    "        P = cvxopt.matrix(np.outer(targets,targets) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(self.N) * -1)\n",
    "        A = cvxopt.matrix(targets, (1,self.N))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(self.N) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(self.N))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(self.N) * -1)\n",
    "            tmp2 = np.identity(self.N)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(self.N)\n",
    "            tmp2 = np.ones(self.N) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # Call the the quadratic solver of cvxopt library.\n",
    "        sol = cvxopt.solvers.qp(cvxopt.matrix(P), cvxopt.matrix(q), cvxopt.matrix(\n",
    "            G), cvxopt.matrix(h), cvxopt.matrix(A), cvxopt.matrix(b))\n",
    "\n",
    "        # Get the Lagrange multipliers out of the solution dictionary\n",
    "        lambdas = np.array(sol['x'])\n",
    "\n",
    "        # Find the (indices of the) support vectors, which are the vectors with non-zero Lagrange multipliers\n",
    "        self.sv = np.where(lambdas > self.threshold)[0]\n",
    "        self.nsupport = len(self.sv)\n",
    "        print (\"Number of support vectors = \", self.nsupport)\n",
    "\n",
    "        # Keep the data corresponding to the support vectors\n",
    "        self.X = X[self.sv, :]\n",
    "        self.lambdas = lambdas[self.sv]\n",
    "        self.targets = targets[self.sv]\n",
    "\n",
    "        self.b = np.sum(self.targets)\n",
    "        for n in range(self.nsupport):\n",
    "            self.b -= np.sum(self.lambdas*self.targets *\n",
    "                             np.reshape(self.K[self.sv[n], self.sv], (self.nsupport, 1)))\n",
    "        self.b /= len(self.lambdas)\n",
    "\n",
    "        if self.kernel == 'poly':\n",
    "            def classify(Y, soft=False):\n",
    "                K = (1. + 1./self.sigma*np.dot(Y, self.X.T))**self.degree\n",
    "\n",
    "                self.y = np.zeros((np.shape(Y)[0], 1))\n",
    "                for j in range(np.shape(Y)[0]):\n",
    "                    for i in range(self.nsupport):\n",
    "                        self.y[j] += self.lambdas[i]*self.targets[i]*K[j, i]\n",
    "                    self.y[j] += self.b\n",
    "\n",
    "                if soft:\n",
    "                    return self.y\n",
    "                else:\n",
    "                    return np.sign(self.y)\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            def classify(Y, soft=False):\n",
    "                K = np.dot(Y, self.X.T)\n",
    "                c = (1./self.sigma * np.sum(Y**2, axis=1)\n",
    "                     * np.ones((1, np.shape(Y)[0]))).T\n",
    "                c = np.dot(c, np.ones((1, np.shape(K)[1])))\n",
    "                aa = np.dot(self.xsquared[self.sv],\n",
    "                            np.ones((1, np.shape(K)[0]))).T\n",
    "                K = K - 0.5*c - 0.5*aa\n",
    "                K = np.exp(K/(2.*self.sigma**2))\n",
    "\n",
    "                self.y = np.zeros((np.shape(Y)[0], 1))\n",
    "                for j in range(np.shape(Y)[0]):\n",
    "                    for i in range(self.nsupport):\n",
    "                        self.y[j] += self.lambdas[i]*self.targets[i]*K[j, i]\n",
    "                    self.y[j] += self.b\n",
    "\n",
    "                if soft:\n",
    "                    return self.y\n",
    "                else:\n",
    "                    return np.sign(self.y)\n",
    "        else:\n",
    "            print (\"Error: Invalid kernel\")\n",
    "            return\n",
    "\n",
    "        self.classify = classify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question:}$ How $b$ was computed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b=1/n(\\sum_{i=1}^{n}y_{i}-\\sum_{i=1}^{n}\\Lambda _{i}y_{i} K(x_{i}.x))\\\\\n",
    "where, n= No.\\:of \\: Support\\:Vectors\\\\\n",
    "x_{i},\\:\\Lambda _{i},\\:y_{i}\\: are\\: elements\\:corresponding\\: to \\:support \\:vectors\\: only$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Classifier\n",
    "In the following, we will now test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "iris = np.loadtxt('iris_data', delimiter=\",\")\n",
    "imax = np.concatenate((iris.max(axis=0)*np.ones((1, 5)),\n",
    "                       iris.min(axis=0)*np.ones((1, 5))), axis=0).max(axis=0)\n",
    "\n",
    "target = -np.ones((np.shape(iris)[0], 3), dtype=float)\n",
    "indices = np.where(iris[:, 4] == 0)\n",
    "target[indices, 0] = 1.\n",
    "indices = np.where(iris[:, 4] == 1)\n",
    "target[indices, 1] = 1.\n",
    "indices = np.where(iris[:, 4] == 2)\n",
    "target[indices, 2] = 1.\n",
    "\n",
    "train = iris[::2, 0:4]\n",
    "traint = target[::2]\n",
    "test = iris[1::2, 0:4]\n",
    "testt = target[1::2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.0447e+00 -6.5298e+00  1e+02  9e+00  1e+00\n",
      " 1: -1.7719e-01 -7.1809e+00  7e+00  1e-15  5e-16\n",
      " 2: -1.9398e+00 -3.0610e+00  1e+00  2e-16  5e-16\n",
      " 3: -2.2490e+00 -2.5784e+00  3e-01  2e-16  3e-16\n",
      " 4: -2.3973e+00 -2.4954e+00  1e-01  8e-16  2e-16\n",
      " 5: -2.4501e+00 -2.4890e+00  4e-02  7e-16  3e-16\n",
      " 6: -2.4732e+00 -2.4755e+00  2e-03  1e-15  3e-16\n",
      " 7: -2.4751e+00 -2.4751e+00  4e-05  1e-16  3e-16\n",
      " 8: -2.4751e+00 -2.4751e+00  8e-07  1e-15  3e-16\n",
      "Optimal solution found.\n",
      "Number of support vectors =  9\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.2621e+01 -3.5616e+01  2e+02  1e+01  2e+00\n",
      " 1: -2.7390e+01 -5.5045e+01  1e+02  6e+00  1e+00\n",
      " 2: -1.2277e+02 -1.6466e+02  1e+02  5e+00  9e-01\n",
      " 3: -2.8016e+02 -3.5690e+02  1e+02  4e+00  7e-01\n",
      " 4: -4.1845e+02 -5.0547e+02  1e+02  1e+00  3e-01\n",
      " 5: -4.2442e+02 -4.4820e+02  3e+01  3e-01  6e-02\n",
      " 6: -4.1895e+02 -4.2821e+02  9e+00  8e-14  5e-14\n",
      " 7: -4.2386e+02 -4.2599e+02  2e+00  8e-14  5e-14\n",
      " 8: -4.2532e+02 -4.2563e+02  3e-01  8e-14  6e-14\n",
      " 9: -4.2557e+02 -4.2558e+02  1e-02  2e-13  6e-14\n",
      "10: -4.2557e+02 -4.2557e+02  3e-04  2e-13  6e-14\n",
      "Optimal solution found.\n",
      "Number of support vectors =  12\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.2131e+01 -3.3571e+01  2e+02  1e+01  2e+00\n",
      " 1: -2.5849e+01 -5.0948e+01  1e+02  5e+00  1e+00\n",
      " 2: -1.2385e+02 -1.5999e+02  1e+02  4e+00  8e-01\n",
      " 3: -3.0074e+02 -3.7416e+02  1e+02  3e+00  7e-01\n",
      " 4: -4.1242e+02 -4.6088e+02  7e+01  9e-01  2e-01\n",
      " 5: -4.0541e+02 -4.1357e+02  1e+01  1e-01  2e-02\n",
      " 6: -4.0516e+02 -4.0545e+02  4e-01  4e-03  7e-04\n",
      " 7: -4.0516e+02 -4.0517e+02  9e-03  7e-05  1e-05\n",
      " 8: -4.0517e+02 -4.0517e+02  2e-04  8e-07  2e-07\n",
      " 9: -4.0517e+02 -4.0517e+02  2e-06  8e-09  2e-09\n",
      "Optimal solution found.\n",
      "Number of support vectors =  7\n"
     ]
    }
   ],
   "source": [
    "# Training the machines\n",
    "output = np.zeros((np.shape(test)[0], 3))\n",
    "\n",
    "# Train for the first set of train data\n",
    "#svm0 = svm(kernel='linear')\n",
    "#svm0 = svm(kernel='linear')\n",
    "#svm0 = svm.svm(kernel='poly',C=0.1,degree=1)\n",
    "svm0 = svm(kernel='rbf')\n",
    "svm0.train_kernel_svm(train, np.reshape(traint[:, 0], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 0] = svm0.classify(test, soft=True).T\n",
    "\n",
    "# Train for the second set of train data\n",
    "#svm1 = svm(kernel='linear')\n",
    "#svm1 = svm(kernel='linear')\n",
    "#svm1 = svm(kernel='poly',degree=3)\n",
    "svm1 = svm(kernel='rbf')\n",
    "svm1.train_kernel_svm(train, np.reshape(traint[:, 1], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 1] = svm1.classify(test, soft=True).T\n",
    "\n",
    "# Train for the third set of train data\n",
    "#svm2 = svm(kernel='linear')\n",
    "#svm2 = svm(kernel='linear')\n",
    "#svm2 = svm(kernel='poly',C=0.1,degree=1)\n",
    "svm2 = svm(kernel='rbf')\n",
    "svm2.train_kernel_svm(train, np.reshape(traint[:, 2], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 2] = svm2.classify(test, soft=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2\n",
      " 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2.]\n",
      "Misclassified locations:\n",
      "[41 58 64 65 66]\n",
      "0.9333333333333333 test accuracy\n"
     ]
    }
   ],
   "source": [
    "# Make a decision about which class\n",
    "# Pick the one with the largest margin\n",
    "bestclass = np.argmax(output, axis=1)\n",
    "print (bestclass)\n",
    "print (iris[1::2, 4])\n",
    "print(\"Misclassified locations:\")\n",
    "err = np.where(bestclass != iris[1::2, 4])[0]\n",
    "print (err)\n",
    "print (float(np.shape(testt)[0] - len(err)) /\n",
    "       (np.shape(testt)[0]), \"test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.6392e+00 -1.1380e+01  3e+02  2e+01  2e-14\n",
      " 1: -8.6211e-01 -1.0232e+01  3e+01  9e-01  2e-14\n",
      " 2: -3.6380e-01 -3.8907e+00  5e+00  9e-02  5e-15\n",
      " 3: -3.0110e-01 -7.2795e-01  5e-01  7e-03  4e-15\n",
      " 4: -3.6339e-01 -5.7795e-01  2e-01  3e-03  4e-15\n",
      " 5: -3.9891e-01 -4.7934e-01  8e-02  1e-03  3e-15\n",
      " 6: -4.1878e-01 -4.3975e-01  2e-02  2e-04  3e-15\n",
      " 7: -4.2608e-01 -4.2915e-01  3e-03  1e-05  3e-15\n",
      " 8: -4.2736e-01 -4.2746e-01  1e-04  3e-07  3e-15\n",
      " 9: -4.2740e-01 -4.2741e-01  1e-06  4e-09  4e-15\n",
      "10: -4.2740e-01 -4.2740e-01  1e-08  4e-11  3e-15\n",
      "Optimal solution found.\n",
      "Number of support vectors =  8\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0352e+01 -2.1688e+01  2e+02  1e+01  2e+00\n",
      " 1: -1.6035e+01 -1.3674e+01  8e+01  5e+00  7e-01\n",
      " 2: -6.8857e+00 -2.9202e+00  2e+01  1e+00  2e-01\n",
      " 3: -1.6160e+00 -6.2213e-01  5e+00  2e-01  3e-02\n",
      " 4: -5.1151e-01 -3.0758e-01  1e+00  4e-02  6e-03\n",
      " 5: -3.6559e-01 -2.2011e-01  5e-01  2e-02  3e-03\n",
      " 6: -2.3875e-01 -1.4036e-01  1e-01  6e-03  8e-04\n",
      " 7: -1.6567e-01 -1.2920e-01  5e-02  2e-03  3e-04\n",
      " 8: -1.0770e-01 -1.2305e-01  2e-02  1e-04  1e-05\n",
      " 9: -1.1974e-01 -1.2063e-01  1e-03  3e-06  5e-07\n",
      "10: -1.2025e-01 -1.2048e-01  3e-04  8e-07  1e-07\n",
      "11: -1.2045e-01 -1.2046e-01  1e-05  5e-09  7e-10\n",
      "12: -1.2046e-01 -1.2046e-01  1e-07  5e-11  1e-11\n",
      "13: -1.2046e-01 -1.2046e-01  1e-09  5e-13  1e-11\n",
      "Optimal solution found.\n",
      "Number of support vectors =  6\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4442e+01 -1.4120e+01  4e+02  2e+01  5e-14\n",
      " 1: -2.8536e+00 -1.3044e+01  2e+01  6e-01  4e-14\n",
      " 2: -2.3340e+00 -6.1641e+00  5e+00  1e-01  1e-14\n",
      " 3: -2.0576e+00 -2.8440e+00  1e+00  2e-02  2e-14\n",
      " 4: -2.2025e+00 -2.3523e+00  2e-01  3e-03  1e-14\n",
      " 5: -2.2448e+00 -2.2780e+00  4e-02  4e-04  1e-14\n",
      " 6: -2.2560e+00 -2.2610e+00  6e-03  6e-05  1e-14\n",
      " 7: -2.2580e+00 -2.2582e+00  2e-04  8e-07  1e-14\n",
      " 8: -2.2581e+00 -2.2581e+00  2e-06  8e-09  1e-14\n",
      "Optimal solution found.\n",
      "Number of support vectors =  31\n"
     ]
    }
   ],
   "source": [
    "# Training the machines\n",
    "output = np.zeros((np.shape(test)[0], 3))\n",
    "\n",
    "# Train for the first set of train data\n",
    "#svm0 = svm(kernel='linear')\n",
    "#svm0 = svm(kernel='linear')\n",
    "svm0 = svm(kernel='poly',C=0.1,degree=1)\n",
    "#svm0 = svm(kernel='rbf')\n",
    "svm0.train_kernel_svm(train, np.reshape(traint[:, 0], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 0] = svm0.classify(test, soft=True).T\n",
    "\n",
    "# Train for the second set of train data\n",
    "#svm1 = svm(kernel='linear')\n",
    "#svm1 = svm(kernel='linear')\n",
    "svm1 = svm(kernel='poly',degree=3)\n",
    "#svm1 = svm(kernel='rbf')\n",
    "svm1.train_kernel_svm(train, np.reshape(traint[:, 1], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 1] = svm1.classify(test, soft=True).T\n",
    "\n",
    "# Train for the third set of train data\n",
    "#svm2 = svm(kernel='linear')\n",
    "#svm2 = svm(kernel='linear')\n",
    "svm2 = svm(kernel='poly',C=0.1,degree=1)\n",
    "#svm2 = svm(kernel='rbf')\n",
    "svm2.train_kernel_svm(train, np.reshape(traint[:, 2], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 2] = svm2.classify(test, soft=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2 2 2 1 2 1 1 1 2 2 2 2 2 2 2\n",
      " 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2.]\n",
      "Misclassified locations:\n",
      "[41 58 62 64 65 66]\n",
      "0.92 test accuracy\n"
     ]
    }
   ],
   "source": [
    "# Make a decision about which class\n",
    "# Pick the one with the largest margin\n",
    "bestclass = np.argmax(output, axis=1)\n",
    "print (bestclass)\n",
    "print (iris[1::2, 4])\n",
    "print(\"Misclassified locations:\")\n",
    "err = np.where(bestclass != iris[1::2, 4])[0]\n",
    "print (err)\n",
    "print (float(np.shape(testt)[0] - len(err)) /\n",
    "       (np.shape(testt)[0]), \"test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.4229e+00 -1.1090e+01  2e+02  2e+01  2e+00\n",
      " 1: -3.3332e+00 -2.5732e+00  3e+01  2e+00  2e-01\n",
      " 2: -7.8545e-01 -1.3664e+00  3e+00  2e-01  2e-02\n",
      " 3: -4.9032e-01 -7.8525e-01  3e-01  4e-03  4e-04\n",
      " 4: -5.9723e-01 -6.9562e-01  1e-01  6e-04  6e-05\n",
      " 5: -6.7485e-01 -6.8573e-01  1e-02  3e-16  9e-15\n",
      " 6: -6.8475e-01 -6.8494e-01  2e-04  5e-16  9e-15\n",
      " 7: -6.8493e-01 -6.8493e-01  2e-06  4e-16  9e-15\n",
      " 8: -6.8493e-01 -6.8493e-01  2e-08  3e-16  9e-15\n",
      "Optimal solution found.\n",
      "Number of support vectors =  2\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.0044e+01 -1.3205e+02  3e+02  1e+01  3e+00\n",
      " 1: -2.3158e+02 -3.0093e+02  9e+01  5e+00  1e+00\n",
      " 2: -9.7819e+02 -1.0943e+03  1e+02  5e+00  1e+00\n",
      " 3: -4.2163e+03 -4.5446e+03  3e+02  4e+00  1e+00\n",
      " 4: -2.8373e+04 -2.9669e+04  1e+03  4e+00  1e+00\n",
      " 5: -5.0886e+05 -5.1489e+05  6e+03  4e+00  1e+00\n",
      " 6: -3.7900e+07 -3.7963e+07  6e+04  4e+00  1e+00\n",
      " 7: -4.2465e+09 -4.2522e+09  6e+06  4e+00  1e+00\n",
      " 8: -4.2886e+09 -4.2944e+09  6e+06  4e+00  1e+00\n",
      " 9: -4.2908e+09 -4.2966e+09  6e+06  4e+00  1e+00\n",
      "10: -5.6250e+09 -5.6326e+09  8e+06  4e+00  1e+00\n",
      "11: -1.6867e+10 -1.6889e+10  2e+07  4e+00  1e+00\n",
      "12: -2.4927e+10 -2.4961e+10  3e+07  4e+00  1e+00\n",
      "13: -1.0402e+11 -1.0416e+11  1e+08  4e+00  1e+00\n",
      "14: -1.8764e+11 -1.8788e+11  2e+08  4e+00  1e+00\n",
      "15: -3.3886e+11 -3.3920e+11  3e+08  4e+00  1e+00\n",
      "Terminated (singular KKT matrix).\n",
      "Number of support vectors =  75\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.5752e+01 -6.2032e+01  3e+02  1e+01  2e+00\n",
      " 1: -7.9691e+01 -9.6378e+01  1e+02  6e+00  1e+00\n",
      " 2: -1.5883e+02 -1.5568e+02  1e+02  5e+00  9e-01\n",
      " 3: -7.5824e+02 -7.2013e+02  1e+02  5e+00  9e-01\n",
      " 4: -5.4804e+02 -5.4400e+02  4e+02  5e+00  9e-01\n",
      " 5: -9.2679e+02 -8.3292e+02  5e+02  3e+00  6e-01\n",
      " 6: -3.3178e+02 -4.6389e+02  1e+02  2e-13  6e-12\n",
      " 7: -3.4923e+02 -4.4677e+02  1e+02  3e-13  5e-12\n",
      " 8: -4.1574e+02 -4.1862e+02  3e+00  3e-13  5e-12\n",
      " 9: -4.1820e+02 -4.1823e+02  3e-02  6e-14  6e-12\n",
      "10: -4.1822e+02 -4.1822e+02  3e-04  9e-14  6e-12\n",
      "Optimal solution found.\n",
      "Number of support vectors =  5\n"
     ]
    }
   ],
   "source": [
    "# Training the machines\n",
    "output = np.zeros((np.shape(test)[0], 3))\n",
    "\n",
    "# Train for the first set of train data\n",
    "#svm0 = svm(kernel='linear')\n",
    "svm0 = svm(kernel='linear')\n",
    "#svm0 = svm.svm(kernel='poly',C=0.1,degree=1)\n",
    "#svm0 = svm(kernel='rbf')\n",
    "svm0.train_kernel_svm(train, np.reshape(traint[:, 0], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 0] = svm0.classify(test, soft=True).T\n",
    "\n",
    "# Train for the second set of train data\n",
    "#svm1 = svm(kernel='linear')\n",
    "svm1 = svm(kernel='linear')\n",
    "#svm1 = svm(kernel='poly',degree=3)\n",
    "#svm1 = svm(kernel='rbf')\n",
    "svm1.train_kernel_svm(train, np.reshape(traint[:, 1], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 1] = svm1.classify(test, soft=True).T\n",
    "\n",
    "# Train for the third set of train data\n",
    "#svm2 = svm(kernel='linear')\n",
    "svm2 = svm(kernel='linear')\n",
    "#svm2 = svm(kernel='poly',C=0.1,degree=1)\n",
    "#svm2 = svm(kernel='rbf')\n",
    "svm2.train_kernel_svm(train, np.reshape(traint[:, 2], (np.shape(train[:, :2])[0], 1)))\n",
    "output[:, 2] = svm2.classify(test, soft=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2\n",
      " 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2.]\n",
      "Misclassified locations:\n",
      "[41 66]\n",
      "0.9733333333333334 test accuracy\n"
     ]
    }
   ],
   "source": [
    "# Make a decision about which class\n",
    "# Pick the one with the largest margin\n",
    "bestclass = np.argmax(output, axis=1)\n",
    "print (bestclass)\n",
    "print (iris[1::2, 4])\n",
    "print(\"Misclassified locations:\")\n",
    "err = np.where(bestclass != iris[1::2, 4])[0]\n",
    "print (err)\n",
    "print (float(np.shape(testt)[0] - len(err)) /\n",
    "       (np.shape(testt)[0]), \"test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ The IRIS dataset has three classes. Explain by observing the code above how the two class SVM was modified for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for extending the SVM to 3 classes we design 3 classifiers and trained each to give true output for a pirticular class and false for the rest so together they  can classify the data into 3 categories.The three classifiers are svm0.classify,svm1.classify,svm2.classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Write mathematical expressions for the kernels defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "Linear \\:Kernel: K(x_{i},x_{j})=x_{i}.x_{j}\\\\\n",
    "Gaussian\\:Kernel(rbf):K(x_{i},x_{j})=\\exp(-\\gamma \\left \\| x_{i}-x_{j} \\right \\|^{2})\\\\\n",
    "Polynomial\\:with \\:degree \\,1 \\:Kernel: K(x_{i},x_{j})=(1\\: + x_{i}.x_{j})^{1}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}:$ Play with different Kernels. Which kernels (polynomial, RBF, or polynomial) give the best test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the kernels are giving different accuracies for the given problem.\n",
    "RBF kernel- 93.33% accuracy\n",
    "Polynomial kernel- 92% accuracy\n",
    "Linear kernel- 97.33% accuracy\n",
    "\n",
    "Hence Linear kernel gives the best accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
