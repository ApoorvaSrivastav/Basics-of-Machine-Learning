{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwGWNHs2xIsx"
   },
   "source": [
    "# Logistic Regression Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcaBeePgu_Tu"
   },
   "source": [
    "## Multi-class classification of MNIST using Logistic Regression\n",
    "\n",
    "The multi-class scenario for logistic regression is quite similar to the binary case, except that the label $y$ is now an integer in {1, ...., K} where $K$ is the number of classes. In this excercise you will be provided with handwritten digit images. Write the code and compute the test accuracy by training a logistic regression based classifier in (i) one-vs-one, and (ii) one-vs-all setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running importer\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        #print('searching: %s'%nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        #print('searching: %s' % nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        #print('Found %d cells'%len(nb.cells))\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "\n",
    "#  register the NotebookFinder with sys.meta_path\n",
    "print('running importer')\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9944,
     "status": "ok",
     "timestamp": 1596983406360,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "ManRVu7IsIjp",
    "outputId": "b48dd937-f2d5-4762-af1a-44fa03c44d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from utils import plot_decision_boundary, get_accuracy, get_prediction\n",
    "from utils import plot_2D_input_datapoints, generate_gifs, sigmoid, normalize\n",
    "import math\n",
    "import gif\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9918,
     "status": "ok",
     "timestamp": 1596983406361,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "xbV2U06Cs45b"
   },
   "outputs": [],
   "source": [
    "# Let's initialize our weights using uniform distribution\n",
    "def weight_init_uniform_dist(X, y):\n",
    "  \n",
    "    np.random.seed(312)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    _, n_outputs = np.shape(y)\n",
    "\n",
    "    limit = 1 / math.sqrt(n_features)\n",
    "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
    "    weights[-1] = 0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36195,
     "status": "ok",
     "timestamp": 1596983432936,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "SAAbK03fLCR1"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "# One hot encoding of our output label vector y\n",
    "def one_hot(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b\n",
    "\n",
    "# Loading dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "Y = digits.target\n",
    "Y = one_hot(Y)\n",
    "\n",
    "# Absorbing weight b of the hyperplane\n",
    "X = digits.data\n",
    "b_ones = np.ones((len(X), 1))\n",
    "X = np.hstack((X, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36177,
     "status": "ok",
     "timestamp": 1596983432939,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "yzdjTbEYLvPK",
    "outputId": "76ed5c87-3298-433d-cf76-d68026c46342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_orig()\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[10])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36148,
     "status": "ok",
     "timestamp": 1596983432942,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "3CIYTv4x65As",
    "outputId": "d9f59ee0-8392-4ba9-8cb8-11afc1669fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (1308, 65)\n",
      "Validation dataset:  (188, 65)\n",
      "Test dataset:  (301, 65)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, val, and test set.\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.167)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape)\n",
    "print(\"Validation dataset: \", X_val.shape)\n",
    "print(\"Test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36132,
     "status": "ok",
     "timestamp": 1596983432945,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "d3NzkO4s68RX"
   },
   "outputs": [],
   "source": [
    "# Normalizing X_train and absorbing weight b of the hyperplane\n",
    "X_normalized_train = normalize(X_train[:, :64])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_train), 1))\n",
    "X_normalized_train = np.hstack((X_normalized_train, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36096,
     "status": "ok",
     "timestamp": 1596983432947,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "pYrK4fK3iyyk",
    "outputId": "a73d5605-db30-4099-f72e-9cca8e2fe3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize our weights using uniform distribution\n",
    "def weight_init_uniform_dist(X, y):\n",
    "  \n",
    "    np.random.seed(312)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    _, n_outputs = np.shape(y)\n",
    "\n",
    "    limit = 1 / math.sqrt(n_features)\n",
    "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
    "    weights[-1] = 0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the dataset\n",
    "def train_binary_classifier(X_train, Y_train, weights, num_epochs=1000, learning_rate=0.1):\n",
    "\n",
    "    \"\"\" Method to train a logistic regression model.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "\n",
    "    X_train: ndarray (num_examples(rows) vs num_columns(columns))\n",
    "    Input data on which the logistic regression model will be trained \n",
    "    to acquire optimal weights\n",
    "\n",
    "    Y_train: ndarray (num_examples(rows) vs class_labels(columns))\n",
    "    Class labels of training set\n",
    "\n",
    "    weights: ndarray (num_features vs n_output)\n",
    "    Weights used to train the network and predict on test set\n",
    "\n",
    "    num_epochs: int\n",
    "    Number of epochs you want to train the model\n",
    "\n",
    "    learning_rate: int\n",
    "    rate with which weights will be update every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = np.shape(X_train)\n",
    "    _, n_outputs = np.shape(Y_train)\n",
    "\n",
    "    history_weights = []\n",
    "    epoch = 1\n",
    "\n",
    "    # Training using Batch GD\n",
    "    while epoch <= num_epochs:\n",
    "\n",
    "        # Appending previous history weights/first initialized weights\n",
    "        history_weights.append(weights)\n",
    "\n",
    "        # Computing weighted inputs and predicting output\n",
    "        w_transpose_x = np.dot(X_train, weights)\n",
    "        y_pred = sigmoid(w_transpose_x)\n",
    "\n",
    "        # Calculating gradient and updating weights\n",
    "        gradient = 1 * np.dot(X_train.T, (Y_train - y_pred))\n",
    "        weights += (learning_rate/n_samples) * gradient\n",
    "        weights = np.round(weights, decimals=7)\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Training complete\")\n",
    "\n",
    "    return history_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights from uniform distribution\n",
    "weights = weight_init_uniform_dist(X_normalized_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the class pair: 0 & 1\n",
      "Training complete\n",
      "Evaluation results for classes (0, 1)\n",
      "Training accuracy: 98.467\n",
      "Validation accuracy: 80.000\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 0 & 2\n",
      "Training complete\n",
      "Evaluation results for classes (0, 2)\n",
      "Training accuracy: 97.674\n",
      "Validation accuracy: 71.429\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 0 & 3\n",
      "Training complete\n",
      "Evaluation results for classes (0, 3)\n",
      "Training accuracy: 99.206\n",
      "Validation accuracy: 71.053\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 0 & 4\n",
      "Training complete\n",
      "Evaluation results for classes (0, 4)\n",
      "Training accuracy: 98.485\n",
      "Validation accuracy: 61.538\n",
      "Test accuracy: 98.438\n",
      "For the class pair: 0 & 5\n",
      "Training complete\n",
      "Evaluation results for classes (0, 5)\n",
      "Training accuracy: 98.092\n",
      "Validation accuracy: 78.788\n",
      "Test accuracy: 94.915\n",
      "For the class pair: 0 & 6\n",
      "Training complete\n",
      "Evaluation results for classes (0, 6)\n",
      "Training accuracy: 98.092\n",
      "Validation accuracy: 62.857\n",
      "Test accuracy: 98.333\n",
      "For the class pair: 0 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (0, 7)\n",
      "Training accuracy: 99.206\n",
      "Validation accuracy: 78.788\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 0 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (0, 8)\n",
      "Training accuracy: 98.819\n",
      "Validation accuracy: 81.250\n",
      "Test accuracy: 98.246\n",
      "For the class pair: 0 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (0, 9)\n",
      "Training accuracy: 97.378\n",
      "Validation accuracy: 92.500\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 1 & 2\n",
      "Training complete\n",
      "Evaluation results for classes (1, 2)\n",
      "Training accuracy: 90.114\n",
      "Validation accuracy: 60.976\n",
      "Test accuracy: 88.889\n",
      "For the class pair: 1 & 3\n",
      "Training complete\n",
      "Evaluation results for classes (1, 3)\n",
      "Training accuracy: 92.607\n",
      "Validation accuracy: 64.103\n",
      "Test accuracy: 98.529\n",
      "For the class pair: 1 & 4\n",
      "Training complete\n",
      "Evaluation results for classes (1, 4)\n",
      "Training accuracy: 94.424\n",
      "Validation accuracy: 60.465\n",
      "Test accuracy: 96.610\n",
      "For the class pair: 1 & 5\n",
      "Training complete\n",
      "Evaluation results for classes (1, 5)\n",
      "Training accuracy: 96.629\n",
      "Validation accuracy: 73.684\n",
      "Test accuracy: 90.741\n",
      "For the class pair: 1 & 6\n",
      "Training complete\n",
      "Evaluation results for classes (1, 6)\n",
      "Training accuracy: 93.258\n",
      "Validation accuracy: 57.895\n",
      "Test accuracy: 90.909\n",
      "For the class pair: 1 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (1, 7)\n",
      "Training accuracy: 98.444\n",
      "Validation accuracy: 62.500\n",
      "Test accuracy: 98.438\n",
      "For the class pair: 1 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (1, 8)\n",
      "Training accuracy: 77.220\n",
      "Validation accuracy: 67.742\n",
      "Test accuracy: 69.231\n",
      "For the class pair: 1 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (1, 9)\n",
      "Training accuracy: 90.441\n",
      "Validation accuracy: 79.545\n",
      "Test accuracy: 84.615\n",
      "For the class pair: 2 & 3\n",
      "Training complete\n",
      "Evaluation results for classes (2, 3)\n",
      "Training accuracy: 90.157\n",
      "Validation accuracy: 66.667\n",
      "Test accuracy: 92.647\n",
      "For the class pair: 2 & 4\n",
      "Training complete\n",
      "Evaluation results for classes (2, 4)\n",
      "Training accuracy: 98.496\n",
      "Validation accuracy: 59.524\n",
      "Test accuracy: 96.610\n",
      "For the class pair: 2 & 5\n",
      "Training complete\n",
      "Evaluation results for classes (2, 5)\n",
      "Training accuracy: 99.621\n",
      "Validation accuracy: 78.125\n",
      "Test accuracy: 96.296\n",
      "For the class pair: 2 & 6\n",
      "Training complete\n",
      "Evaluation results for classes (2, 6)\n",
      "Training accuracy: 99.242\n",
      "Validation accuracy: 59.459\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 2 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (2, 7)\n",
      "Training accuracy: 97.244\n",
      "Validation accuracy: 69.444\n",
      "Test accuracy: 96.875\n",
      "For the class pair: 2 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (2, 8)\n",
      "Training accuracy: 91.016\n",
      "Validation accuracy: 78.788\n",
      "Test accuracy: 88.462\n",
      "For the class pair: 2 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (2, 9)\n",
      "Training accuracy: 95.167\n",
      "Validation accuracy: 75.000\n",
      "Test accuracy: 94.231\n",
      "For the class pair: 3 & 4\n",
      "Training complete\n",
      "Evaluation results for classes (3, 4)\n",
      "Training accuracy: 98.846\n",
      "Validation accuracy: 65.000\n",
      "Test accuracy: 98.630\n",
      "For the class pair: 3 & 5\n",
      "Training complete\n",
      "Evaluation results for classes (3, 5)\n",
      "Training accuracy: 93.023\n",
      "Validation accuracy: 75.000\n",
      "Test accuracy: 97.059\n",
      "For the class pair: 3 & 6\n",
      "Training complete\n",
      "Evaluation results for classes (3, 6)\n",
      "Training accuracy: 99.612\n",
      "Validation accuracy: 62.857\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 3 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (3, 7)\n",
      "Training accuracy: 97.177\n",
      "Validation accuracy: 65.714\n",
      "Test accuracy: 98.718\n",
      "For the class pair: 3 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (3, 8)\n",
      "Training accuracy: 89.200\n",
      "Validation accuracy: 74.194\n",
      "Test accuracy: 93.939\n",
      "For the class pair: 3 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (3, 9)\n",
      "Training accuracy: 83.650\n",
      "Validation accuracy: 60.526\n",
      "Test accuracy: 80.303\n",
      "For the class pair: 4 & 5\n",
      "Training complete\n",
      "Evaluation results for classes (4, 5)\n",
      "Training accuracy: 98.148\n",
      "Validation accuracy: 82.143\n",
      "Test accuracy: 96.610\n",
      "For the class pair: 4 & 6\n",
      "Training complete\n",
      "Evaluation results for classes (4, 6)\n",
      "Training accuracy: 98.519\n",
      "Validation accuracy: 66.667\n",
      "Test accuracy: 95.000\n",
      "For the class pair: 4 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (4, 7)\n",
      "Training accuracy: 93.077\n",
      "Validation accuracy: 74.194\n",
      "Test accuracy: 91.304\n",
      "For the class pair: 4 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (4, 8)\n",
      "Training accuracy: 95.038\n",
      "Validation accuracy: 76.923\n",
      "Test accuracy: 94.737\n",
      "For the class pair: 4 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (4, 9)\n",
      "Training accuracy: 97.091\n",
      "Validation accuracy: 79.487\n",
      "Test accuracy: 91.228\n",
      "For the class pair: 5 & 6\n",
      "Training complete\n",
      "Evaluation results for classes (5, 6)\n",
      "Training accuracy: 98.134\n",
      "Validation accuracy: 61.111\n",
      "Test accuracy: 98.182\n",
      "For the class pair: 5 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (5, 7)\n",
      "Training accuracy: 96.512\n",
      "Validation accuracy: 68.571\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 5 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (5, 8)\n",
      "Training accuracy: 93.846\n",
      "Validation accuracy: 91.429\n",
      "Test accuracy: 92.308\n",
      "For the class pair: 5 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (5, 9)\n",
      "Training accuracy: 85.714\n",
      "Validation accuracy: 57.778\n",
      "Test accuracy: 80.769\n",
      "For the class pair: 6 & 7\n",
      "Training complete\n",
      "Evaluation results for classes (6, 7)\n",
      "Training accuracy: 99.612\n",
      "Validation accuracy: 76.471\n",
      "Test accuracy: 100.000\n",
      "For the class pair: 6 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (6, 8)\n",
      "Training accuracy: 95.385\n",
      "Validation accuracy: 86.667\n",
      "Test accuracy: 96.226\n",
      "For the class pair: 6 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (6, 9)\n",
      "Training accuracy: 100.000\n",
      "Validation accuracy: 86.047\n",
      "Test accuracy: 98.113\n",
      "For the class pair: 7 & 8\n",
      "Training complete\n",
      "Evaluation results for classes (7, 8)\n",
      "Training accuracy: 94.400\n",
      "Validation accuracy: 84.375\n",
      "Test accuracy: 98.387\n",
      "For the class pair: 7 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (7, 9)\n",
      "Training accuracy: 95.817\n",
      "Validation accuracy: 90.476\n",
      "Test accuracy: 93.548\n",
      "For the class pair: 8 & 9\n",
      "Training complete\n",
      "Evaluation results for classes (8, 9)\n",
      "Training accuracy: 83.019\n",
      "Validation accuracy: 68.085\n",
      "Test accuracy: 84.000\n",
      "Test accuracies for all pairs: [100.0, 100.0, 100.0, 98.4375, 94.91525423728814, 98.33333333333333, 100.0, 98.24561403508771, 100.0, 88.88888888888889, 98.52941176470588, 96.61016949152543, 90.74074074074075, 90.9090909090909, 98.4375, 69.23076923076923, 84.61538461538461, 92.64705882352942, 96.61016949152543, 96.2962962962963, 100.0, 96.875, 88.46153846153845, 94.23076923076923, 98.63013698630137, 97.05882352941177, 100.0, 98.71794871794873, 93.93939393939394, 80.3030303030303, 96.61016949152543, 95.0, 91.30434782608697, 94.73684210526316, 91.22807017543859, 98.18181818181819, 100.0, 92.3076923076923, 80.76923076923077, 100.0, 96.22641509433963, 98.11320754716981, 98.38709677419355, 93.5483870967742, 84.0]\n",
      "Mean test accuracy of all the pairs 94.49060223102428\n"
     ]
    }
   ],
   "source": [
    "# One vs One Classifier\n",
    "classes=range(Y_train.shape[1])\n",
    "a=[]\n",
    "comb=combinations(classes, 2) #All possible one vs one classifier to be trained\n",
    "\n",
    "for i in list(comb): \n",
    "    #Finding and removing the samples of class other than the pair i for training set\n",
    "    print('For the class pair:',i[0],\"&\",i[1])\n",
    "    binary_y_train= np.empty((Y_train.shape[0]))\n",
    "    binary_y_val= np.empty((Y_val.shape[0]))\n",
    "    binary_y_test= np.empty((Y_test.shape[0]))\n",
    "    for j in range(Y_train.shape[0]):\n",
    "        if Y_train[j,int(i[0])]==1:\n",
    "            binary_y_train[j]=0\n",
    "        elif Y_train[j,int(i[1])]==1:\n",
    "            binary_y_train[j]=1\n",
    "        else:\n",
    "            binary_y_train[j]=-1\n",
    "    other_class=np.argwhere(binary_y_train==-1)\n",
    "    binary_y_train=np.delete(binary_y_train,other_class) \n",
    "    binary_y_train=np.reshape(binary_y_train,(-1,1))\n",
    "    binary_Xnorm_train=np.delete(X_normalized_train,other_class,axis=0)\n",
    "    binary_x_train=np.delete(X_train,other_class,axis=0)\n",
    "    \n",
    "    #Finding and removing the samples of class other than the pair i for training set\n",
    "    for j in range(Y_val.shape[0]):\n",
    "        if Y_val[j,int(i[0])]==1:\n",
    "            binary_y_val[j]=0\n",
    "        elif Y_train[j,int(i[1])]==1:\n",
    "            binary_y_val[j]=1\n",
    "        else:\n",
    "            binary_y_val[j]=-1\n",
    "    other_class=np.argwhere(binary_y_val==-1)\n",
    "    binary_y_val=np.delete(binary_y_val,other_class) \n",
    "    binary_y_val=np.reshape(binary_y_val,(-1,1))\n",
    "    binary_x_val=np.delete(X_val,other_class,axis=0)\n",
    "    \n",
    "    #Finding and removing the samples of class other than the pair i for test set\n",
    "    for j in range(Y_test.shape[0]):\n",
    "        if Y_test[j,int(i[0])]==1:\n",
    "            binary_y_test[j]=0\n",
    "        elif Y_test[j,int(i[1])]==1:\n",
    "            binary_y_test[j]=1\n",
    "        else:\n",
    "            binary_y_test[j]=-1\n",
    "    other_class=np.argwhere(binary_y_test==-1)\n",
    "    binary_y_test=np.delete(binary_y_test,other_class) \n",
    "    binary_y_test=np.reshape(binary_y_test,(-1,1))\n",
    "    binary_x_test=np.delete(X_test,other_class,axis=0)\n",
    "    #print(binary_y_test.shape,binary_x_test.shape)\n",
    "    \n",
    "    #training the binary classifier for the given class pair\n",
    "    trained_weights = train_binary_classifier(binary_Xnorm_train, binary_y_train, weights, num_epochs=100, learning_rate=0.1)\n",
    "    best_weights = trained_weights[-1]\n",
    "\n",
    "    print(\"Evaluation results for classes\",i)\n",
    "    train_acc, _ = get_prediction(binary_x_train, binary_y_train, best_weights, get_acc=True, model_type='logreg', predict='yes')\n",
    "    val_acc, _ = get_prediction(binary_x_val, binary_y_val, best_weights, get_acc=True, model_type='logreg', predict='yes')\n",
    "    test_acc, _ = get_prediction(binary_x_test, binary_y_test, best_weights, get_acc=True, model_type='logreg', predict='yes')\n",
    "    \n",
    "    a.append(test_acc)\n",
    "    print(\"Training accuracy: {:.3f}\" .format(train_acc))\n",
    "    print(\"Validation accuracy: {:.3f}\" .format(val_acc))\n",
    "    print(\"Test accuracy: {:.3f}\" .format(test_acc))\n",
    "print('Test accuracies for all pairs:',a)\n",
    "a=np.array(a)\n",
    "print('Mean test accuracy of all the pairs',np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of training data and validation data\n",
    "def predict_one_vs_all(trained_weights, X_input, Y_input):\n",
    "  \n",
    "    num_classes = Y_input.shape[1]\n",
    "    scores = np.zeros(((X_input.shape)[0],num_classes))\n",
    "\n",
    "    #for k in range(num_classes):\n",
    "    \n",
    "        #binary_y_input = np.where(Y_input == k, 1, 0)\n",
    "    trained_weights=np.array(trained_weights)\n",
    "    w_transpose_x = np.dot(X_input, trained_weights)\n",
    "    y_pred = sigmoid(w_transpose_x)\n",
    "        #y_pred = y_pred.reshape((-1,))\n",
    "        #scores[k, :] = y_pred[:,k]\n",
    "\n",
    "    pred_X = np.argmax(y_pred, axis=1)\n",
    "    for i in range(pred_X.shape[0]):\n",
    "        scores[i,pred_X[i]]=1\n",
    "    #print(scores)\n",
    "    #print(X_input.shape,trained_weights.shape,y_pred.shape,pred_X.shape)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One vs All classifier\n",
    "def train_one_vs_all(X_train, Y_train, base_weights, num_epochs=1000, learning_rate=0.1):\n",
    "\n",
    "    \"\"\"Method to train a multiclass classifier using one vs all technique.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "\n",
    "    X_train: ndarray (num_examples(rows) vs num_columns(columns))\n",
    "    Input data on which the logistic regression model will be trained \n",
    "    to acquire optimal weights\n",
    "\n",
    "    Y_train: ndarray (num_examples(rows) vs class_labels(columns))\n",
    "    Class labels of training set\n",
    "\n",
    "    base_weights: ndarray (num_features vs n_outputs)\n",
    "    Weights used to train the network and predict on test set\n",
    "\n",
    "    num_epochs: int\n",
    "    Number of epochs you want to train the model\n",
    "\n",
    "    learning_rate: int\n",
    "    rate with which weights will be update every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = np.shape(X_train)\n",
    "    _, n_outputs = np.shape(Y_train)\n",
    "\n",
    "    history_weights = []\n",
    "    weights_k_classes = []\n",
    "    epoch = 1\n",
    "\n",
    "    classes = range(Y_train.shape[1])\n",
    "\n",
    "    # Training using Batch GD\n",
    "\n",
    "    weights = base_weights\n",
    "      \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "        # Computing weighted inputs and predicting output\n",
    "        w_transpose_x = np.dot(X_train, weights)  #here all classes are taken care simultaneously\n",
    "        y_pred = sigmoid(w_transpose_x)\n",
    "\n",
    "        # Calculating gradient and updating weights\n",
    "        gradient = np.dot(X_train.T, (Y_train - y_pred))\n",
    "        weights += (learning_rate/n_samples) * gradient\n",
    "        weights= np.round(weights, decimals=7)\n",
    "\n",
    "        #weights_k_classes.append(weights)\n",
    "        epoch += 1\n",
    " \n",
    "        #history_weights.append(weights_k_classes)\n",
    "        history_weights.append(weights)\n",
    "        #weights_k_classes = []\n",
    "\n",
    "    print(\"Training complete\")\n",
    "    return history_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Initializing weights from uniform distribution\n",
    "weights = weight_init_uniform_dist(X_train, Y_train)\n",
    "trained_weights = train_one_vs_all(X_train, Y_train, weights, num_epochs=100, learning_rate=0.1)\n",
    "best_weights = trained_weights[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy: 0.944954\n",
      "Validation dataset accuracy: 0.952128\n",
      "Test dataset accuracy: 0.920266\n"
     ]
    }
   ],
   "source": [
    "pred_train_one_vs_all = predict_one_vs_all(best_weights, X_train, Y_train)\n",
    "#print(Y_train,'and',pred_train_one_vs_all.shape)\n",
    "pred_val_one_vs_all = predict_one_vs_all(best_weights, X_val, Y_val)\n",
    "pred_test_one_vs_all = predict_one_vs_all(best_weights, X_test, Y_test)\n",
    "\n",
    "# pred_train_one_vs_all = pred_train_one_vs_all.reshape((-1, 1))\n",
    "# pred_val_one_vs_all = pred_val_one_vs_all.reshape((-1, 1))\n",
    "# pred_test_one_vs_all = pred_test_one_vs_all.reshape((-1, 1))\n",
    "acc_train=0\n",
    "for i in range(Y_train.shape[0]):\n",
    "    if (Y_train[i]==pred_train_one_vs_all[i]).all():\n",
    "        acc_train+=1\n",
    "acc_train=acc_train/Y_train.shape[0]\n",
    "\n",
    "acc_val=0\n",
    "for i in range(Y_val.shape[0]):\n",
    "    if (Y_val[i]==pred_val_one_vs_all[i]).all():\n",
    "        acc_val+=1\n",
    "acc_val=acc_val/Y_val.shape[0]\n",
    "\n",
    "acc_test=0\n",
    "for i in range(Y_test.shape[0]):\n",
    "    if (Y_test[i]==pred_test_one_vs_all[i]).all():\n",
    "        acc_test+=1\n",
    "acc_test=acc_test/Y_test.shape[0]\n",
    "#print(acc_train,acc_val,acc_test)\n",
    "print('Training dataset accuracy: %f' % (acc_train))\n",
    "print('Validation dataset accuracy: %f' % (acc_val))\n",
    "print('Test dataset accuracy: %f' % (acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LogisticRegression_draft4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
